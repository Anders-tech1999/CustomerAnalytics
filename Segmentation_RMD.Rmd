---
title: "Segmentation_RMD"
output: html_document
date: "2024-04-11"
---

# HBAT dataset
HBAT is a manufacturer of paper products who sells products to two market segments:  - the newsprint industry and 
- the magazine industry. 
The data used in this example is based on a survey of HBAT customers who completed a questionnaire on a website. 
 - 100 customers (purchasing managers from firms) buying from HBAT completed the questionnaire.

   HBAT’s data warehouse (customer size, lenght)
-	The first type of information is available from HBAT’s data warehouse and includes information, such as, size of the customer and length of purchase relationship

   Consumers Perceptions
-	The second type of information was consumers perceptions of HBAT’s performance on 13 attributes using a 0-10 scale with 10 being “Excellent” and 0 being “Poor”

   Purchase Outcomes
-	The third type of information relates to purchase outcomes and business relationships (e.g. satisfaction with HBAT, and whether the firm would consider a strategic alliance/partnership with HBAT)

```{r dataload}
# R script for the analysis of the HBAT example in the cluster analysis lecture
#install.packages("readstata13")
#install.packages("NbClust")

library(readstata13)
library(NbClust)
rm(list=ls())

HBAT <- read.dta13("Datasets/hbat.dta")
#HBAT <- read.dta13("C:/Users/Bruger/OneDrive - Aarhus universitet/8. semester - BI/CA - Customer Analytics/Segmentation/hbat.dta") #This also works
```

nobs: Number of Observations
```{r Variable selection}
# first we make our own id variable based on the original order
nobs <- nrow(HBAT) #calculates the number of observations (rows) in the dataset HBAT and assigns it to the variable nobs.

HBAT$id <- seq(1,nobs) #a new column id is added to the HBAT data frame. It is filled with a sequence of numbers from 1 to nobs, effectively creating a unique identifier for each row based on its original order in the data frame.

# pick the variables for cluster analysis and summarize them
hbat <- c("x6","x8","x12","x15","x18") # a vector
summary(HBAT[,hbat])
```
 o X6: Product Quality
Mean   : 7.810 - Customers rates 7.8 on the Product Quality
Min.   : 5.000 - The lowest score given is 5, which states a relative high "worst performance"
 
 o X8: Technical Support
Mean   : 5.365 - Customers rates 5.3 on the Product Quality
Min.   : 1.300 - The lowest score given is 1.3, which states a quite bad performance experienced by a customer
Max.   : 8.500 - The highest score given is 8.5, which states a relative mediocre "best performance" experienced by a customer

 o X12: Salesforce Image

 o X15: New Products

 o X18: Delivery Speed


```{r SD and Multicollinearity}
apply(HBAT[,hbat],2,sd)

# multicollinearity should not be a problem
cor(HBAT[,hbat])
# which is confirmed
```
 o Standard Deviation
x6        x8       x12       x15       x18 
1.3962793 1.5304568 1.0723198 1.4930478 0.7344372
 - standard deviations provide insights into the variability of each column.
 - x8 Technical Support: 1.5304568 The highest variance in customer experience, which states that the technical capabilities is the most varied compentence within these Business areas

 o Multicollinearity
             x6          x8         x12         x15
x6   1.00000000  0.09560045 -0.15181285  0.02698816
x8   0.09560045  1.00000000  0.01699055 -0.07357899
x12 -0.15181285  0.01699055  1.00000000  0.03164010
x15  0.02698816 -0.07357899  0.03164010  1.00000000
x18  0.02771801  0.02544069  0.27155127  0.10574950
           x18
x6  0.02771801
x8  0.02544069
x12 0.27155127
x15 0.10574950
x18 1.0000000
- Values above 0.7 or 0.8 (or below -0.7 or -0.8) are often considered high, indicating potential multicollinearity problems.
Values between 0.5 and 0.7 (or -0.5 and -0.7) suggest moderate correlation, but they might still be acceptable depending on the analysis.


Outlier detection
o	An object which doesn’t fit into a given pattern may represent: 
 - a true outlier, a small but insignificant segment of the population or 
  - an actual and relevant group which is underrepresented in the sample
```{r Outlier detection}
# look for outliers
dev <- t(t(HBAT[,hbat])-apply(HBAT[,hbat],2,mean))
dev2 <- dev^2
sumdev2 <- rowSums(dev2)
sort(sqrt(sumdev2))
# observations 6 and 87 are candidates for deletion
```
The outliers are assessed in the relationship of the outliers being next to or before in the sequence.
The distance between obs 90 and 6 is 0.68, which is a value giving the analyst the possibility to extract obs 6 from the dataset.

 o Acceptable distance: 0.42
Observation 44 : 4.051562   
Observation 53  : 4.475840   
Obs 44, 53 distance: 4.475840 - 4.051562 = 0.42

 o Deletion distance: 0.68
Observation 90 : 4.620276 
Observation 6  : 5.304446 
Obs 90, 6 distance: 5.304446 - 4.620276 = 0.68

 o Deletion distance: 0.96
Observation 90 : 4.620276 
Observation 87  : 5.575872  
Obs 90, 6 distance: 5.575872  - 4.620276 = 0.96

SUMMARY: Observation 6 and 87 are deleted on the basis of the calculations^



Extracting variable 6 and 87
```{r}
HBAT <- subset(HBAT,id!="6"&id!="87") #extracting variable 6 and 87
```

Rearranging dataframe and rows
```{r}
nobs <- nrow(HBAT) #number of rows in the dataset
HBAT$id <- seq(1,nobs) #number of observations (nobs) is recalculated, and the id column is reinitialized to have a sequential order starting from 1 up to the number of observations.
```



## Ward's method - Hierarchical clustering
-	Complete-linkage and Wards methods are generally preferred

- IT IS RECOOMENDED THAT A COMBINATION OF THE HIERARCHICAL AND NONHIERARCHICAL METHODS IS USED
o	Use the hierarchical methods to get a qualified estimate on the number of clusters
o	Use the nonhierarchical methods to determine the cluster affiliation (tilknytning) for each object

Ward’s method calculates the within-cluster sums of squares for all possible five-cluster solutions:
(I think we get 5 possible cluster solutions having 6 variables, remember for different exercises)
```{r Ward's method - Dendrogram Hierarchical clustering}
## Hierarchical clustering
# first create distance matrix
dist <- dist(HBAT[,hbat],method="euclidean") #dist function calculates the Euclidean distance between observations using the selected variables (hbat). 
#This creates a distance matrix that hierarchical clustering algorithms use to determine how observations group together.
dist2 <- dist^2 #Ward's method is sensitive to the scale of the distances, and squaring the distances emphasizes larger distances.

# Wards method
H.fit <- hclust(dist2,method="ward.D") #Ward's method minimizes the total within-cluster variance.

# draw a dendogram
plot(H.fit) #Assess the number of clusters from this plot
```
 -  cut around the height of 100, it looks like there would be around 4-5 clusters


```{r Wards - pct increase}
# assess pct. increase
denominator <- cumsum(H.fit[[2]]) #calculates the cumulative sum of the heights (or distances) at which each clustering step takes place. In Ward's method, these heights represent the increase in the total within-cluster variance when two clusters are merged. H.fit[[2]] contains the heights of the merges at each step.

length(denominator) <- nobs-2 #length of the denominator vector is set to be one less than the number of observations in the data because the hierarchical clustering process involves n-1 merge steps.

denominator <- c(1,denominator) #A '1' is prepended to the denominator vector. This ensures that when calculating percentage increases, the first step (where no previous clustering has occurred) is properly handled by having a base of '1' to compare against.

pct <- H.fit[[2]]/denominator #calculates the percentage increase in the height at each step of the clustering process by dividing the height of each merge by the cumulative sum up to the previous step. This percentage increase can be interpreted as how much 'worse' the clustering gets at each step, as measured by Ward's distance metric.

tail(pct,n=10) #displays the last 10 values of the pct vector. These values are the percentage increases for the final 10 steps in the hierarchical clustering process.
```

```{r}
0.07366355 - 0.07841626
0.07841626 - 0.08764102
0.08764102 - 0.09618998
0.09618998 - 0.11978783
0.11978783 - 0.11439048
```
from 1 -> 2: 0.07366355 - 0.07841626 = 0.0048 diff
from 2 -> 3: 0.07841626 - 0.08764102 = 0.0092 diff
from 3 -> 4: 0.08764102 - 0.09618998 = 0.0086 diff
from 4 -> 5: 0.09618998 - 0.11978783 = 0.0236 diff ***
from 5 -> 6: 0.11978783 - 0.11439048 = 0.0054 diff

from 4 -> 5 is somewhat significant larger than the other increases... 
 - We stop at 4???!!!


Using Wards method to cut into 4 clusters
```{r wards - create cluster groups}
# apart from going from 2-1 the largest jump is from 4-3, we stop just before
grp <- as.factor(cutree(H.fit,k=4))
table(grp) #majority is in first cluster having 49 obs
```
Distribution among the culsters:
grp
Cluster 1: 49
Cluster 2: 18  
Cluster 3: 14  
Cluster 4: 17


```{r Wards - Dendrogram red borders}
# illustrate a 4 cluster solution
plot(H.fit)
rect.hclust(H.fit,k=4,border="red") #smart plot
```
Count the amount of lines interfering the horizontal upper red line.
 = 4 Clusters!


Assess cluster groups
```{r Wards - Assess cluster groups}
# assess outcome
aggregate(HBAT[,hbat],list(grp),mean)
```
 o x6 Product quality
- scoring highest in Cluster 1: 8.208163
- scoring lowest in Cluster 3: 5.971429

 o x8 Technical Support is
- scoring highest in Cluster 4: 6.464706
- scoring lowest in Cluster 2: 4.044444



ANOVA on variables
```{r Wards - ANOVA on variables}
summary(aov(x6~grp,data=HBAT))
summary(aov(x8~grp,data=HBAT))
summary(aov(x12~grp,data=HBAT))
summary(aov(x15~grp,data=HBAT))
summary(aov(x18~grp,data=HBAT))
```
The ANOVA shows, that all variables except x18 Delivery speed have HIGHLY significant difference within the mean of the groups.

X6: Product Quality
 - eg. x6 has means which differs in the group, as this group may be more volatile due to the scores given in each observation in the group
 - Mean Sq: 19.382

 - eg. x18 has means which is quite close to each other, yeilding a score where the observations are quite similar.
 - Mean Sq: 2.6068



## Complete-linkage method - Hierarchical clustering
-	Complete-linkage and Wards methods are generally preferred

- IT IS RECOMMENDED THAT A COMBINATION OF THE HIERARCHICAL AND NONHIERARCHICAL METHODS IS USED
o	Use the hierarchical methods to get a qualified estimate on the number of clusters
o	Use the nonhierarchical methods to determine the cluster affiliation (tilknytning) for each object

-	Complete-linkage/farthest-neighbor measures the distance between clusters as the maximum of the distance between all possible pairs of objects in the two clusters – tends to find compact clusters with equal diameters
-	These methods only depend on the ordinal properties of the distances
```{r Complete-linkage method - Dendrogram }
# Complete linkage
H.fit <- hclust(dist,method="complete")
# draw a dendogram
plot(H.fit)
```
 -  cut around the height of 6-7, it looks like there would be around 4-5 clusters



```{r Complete - pct increase }
# assess pct. increase
denominator <- cumsum(H.fit[[2]]) #calculates the cumulative sum of the heights (or distances) at which each clustering step takes place. In Ward's method, these heights represent the increase in the total within-cluster variance when two clusters are merged. H.fit[[2]] contains the heights of the merges at each step.

length(denominator) <- nobs-2 #length of the denominator vector is set to be one less than the number of observations in the data because the hierarchical clustering process involves n-1 merge steps.

denominator <- c(1,denominator) #A '1' is prepended to the denominator vector. This ensures that when calculating percentage increases, the first step (where no previous clustering has occurred) is properly handled by having a base of '1' to compare against.

pct <- H.fit[[2]]/denominator #calculates the percentage increase in the height at each step of the clustering process by dividing the height of each merge by the cumulative sum up to the previous step. This percentage increase can be interpreted as how much 'worse' the clustering gets at each step, as measured by Ward's distance metric.

tail(pct,n=10) ##displays the last 10 values of the pct vector. These values are the percentage increases for the final 10 steps in the hierarchical clustering process.
```

```{r}
0.02677534 - 0.02682930
0.02682930 - 0.0268346
0.02683468 - 0.02898199
0.02898199 - 0.02950276
0.02950276 - 0.03010020
0.03010020 - 0.02968177
0.02968177 - 0.03498446
0.03498446 - 0.03620317
0.03620317 - 0.03714618
```
from 1 -> 2: 0.02677534 - 0.02682930 = -5.396e-05
from 2 -> 3: 0.02682930 - 0.0268346  = -5.3e-06
from 3 -> 4: 0.02683468 - 0.02898199 = -0.00214731
from 4 -> 5: 0.02898199 - 0.02950276 = -0.00052077
from 5 -> 6: 0.02950276 - 0.03010020 = -0.00059744
from 6 -> 7: 0.03010020 - 0.02968177 = 0.00041843


```{r Plot Percentage increase}
# Example values from the provided output
pct <- c(0.02677534, 0.02682930, 0.02683468, 0.02898199, 0.02950276, 
         0.03010020, 0.02968177, 0.03498446, 0.03620137, 0.03741618)

# Plotting the percentage increases
plot(pct, type="b", xlab="Step", ylab="Percentage Increase", main="Percentage Increase at Each Step")
```
Use this plot or calculate the increases manually?
from 3 -> 4 is somewhat significant larger than the other increases... 
 - We stop at 3 (This is different from Wards method)


Complete Linkage to cut into 4 clusters
```{r Complete Linkage - create cluster groups }
# results from Ward's method are supported -
grp <- as.factor(cutree(H.fit,k=3))
table(grp) #majority is in second cluster having 33 obs
```
Distribution among the culsters:
grp
Cluster 1: 48
Cluster 2: 41  
Cluster 3: 9  



```{r Complete - Dendrogram red borders }
# but size of the clusters differ
plot(H.fit)
rect.hclust(H.fit,k=3,border="red")
```
This tends to a 3 cluster solution???


```{r Complete - Assess cluster groups }
# assess outcome
aggregate(HBAT[,hbat],list(grp),mean)
```
 o x6 Product quality
- scoring highest in Cluster 3
- scoring lowest in Cluster 1

 o x8 Technical Support
- scoring highest in Cluster 2
- scoring lowest in Cluster 3


Complete Linkage - ANOVA on variables
```{r Complete Linkage - Dendrogram red borders }
summary(aov(x6~grp,data=HBAT))
summary(aov(x8~grp,data=HBAT))
summary(aov(x12~grp,data=HBAT))
summary(aov(x15~grp,data=HBAT))
summary(aov(x18~grp,data=HBAT))
```
The ANOVA shows, that all variables except x18 Delivery speed have significant difference within the mean of the groups.
eg. x6 has means which differs in the group, as this group may be more volatile due to the scores given in each observation in the group
eg. x18 has means which is quite close to each other, yeilding a score where the observations are quite similar

The ANOVA shows, that all variables
 - except x18 DeliverySpeed  
 - expect X12: SalesforceImage 
 - have HIGHLY significant difference within the mean of the groups.

 o X6: Product Quality
 - eg. x6 has means which differs in the group, as this group may be more volatile due to the scores given in each observation in the group
 - Mean Sq: 6.104

 o x18 DeliverySpeed 
 - eg. x18 has means which is quite close to each other, yeilding a score where the observations are quite similar.
 - Mean Sq: 0.8802



- IT IS RECOOMENDED THAT A COMBINATION OF THE HIERARCHICAL AND NONHIERARCHICAL METHODS IS USED
o	Use the hierarchical methods to get a qualified estimate on the number of clusters
o	Use the nonhierarchical methods to determine the cluster affiliation (tilknytning) for each object
```{r NbClust }
# use NbClust
res<-NbClust(HBAT[,hbat], 
             distance = "euclidean", 
             min.nc=2, #minimum clusters to consider is 2
             max.nc=8, #maximum clusters to consider is 8
             method = "ward.D", #Ward minimizes the total within-cluster variance.
             index = "all") #go with the printed number of clusters
res
#res$All.index NOT NECESSARY
#res$Best.nc NOT NECESSARY
```
This code actually recommends a majority answer: Go primarily with THIS
* Among all indices:                                                
* 7 proposed 2 as the best number of clusters 
* 3 proposed 3 as the best number of clusters 
* 5 proposed 4 as the best number of clusters 
* 2 proposed 6 as the best number of clusters 
* 6 proposed 8 as the best number of clusters 
                   ***** Conclusion *****                       
* the majority rule, the best number of clusters is  2




#Non-hierarchical clusters

FROM HERE 4 CLUSTERS IS USED AGAIN - FROM WARDS RECOMMENDATION

-	The general purpose of nonhierarchical clustering is to classify objects into k clusters such that
o The objects within the same cluster are as similar as possible – high intra-class similarity
o The objects from different clusters are as dissimilar as possible – low inter-class similarity

There are two key differences between nonhierarchical and hierarchical clustering
o	1 The number of clusters, k, must be known a priori
o	2 Once an object has been assigned a particular cluster it can later in the process be assigned a different cluster - thus the treelike construction process doesn’t apply


```{r Complete - Dendrogram red borders }
## Nonhierarchical clustering
set.seed(4118)
NH.fit<-kmeans(HBAT[,hbat],4,nstart=25) #nstart=25 means that the algorithm will be run 25 times with different random starting assignments, and the best result (with the lowest within-cluster sum of squares) will be chosen.
print(NH.fit)
```
K-means clustering with 4 clusters of sizes 17, 22, 28, 31

Cluster means:
        x6       x8      x12      x15      x18
1 8.182353 3.923529 5.488235 6.752941 4.052941
2 8.872727 6.959091 4.804545 5.686364 3.854545
3 6.171429 5.957143 5.560714 4.946429 3.957143
4 8.464516 4.693548 4.806452 3.854839 3.796774

 o X6: Product Quality:
 - scores high in cluster 1 and 2 (relatively low in cluster 3)
 - Has highest average scores across all clusters in comparison with other variables
X8: Technical Support
X12: Salesforce Image
X15: New Products
X18: Delivery Speed

Clustering vector:
  1   2   3   4   5  
  4   4   4   3   4 
 - 1st Observation is assigned to Cluster 4 ...
 - 4th Observation is assigned to Cluster 3



Cluster quantity distribution
```{r Complete - Dendrogram red borders }
grp <- as.factor(NH.fit[[1]])
table(grp)
```
Distribution among the culsters:
grp
Cluster 1: 17
Cluster 2: 22  
Cluster 3: 28  
Cluster 4: 31



ANOVA (Analysis of Variance) tests conducted on the variables x6, x8, x12, x15, and x18 to 
 - determine if there are significant differences in their means across the clusters formed by the k-means clustering.
```{r Complete - Dendrogram red borders }
# assess outcome
#aggregate(HBAT[,hbat],list(grp),mean) THIS IS ALREADY LISTED ABOVE
summary(aov(x6~grp,data=HBAT))
summary(aov(x8~grp,data=HBAT))
summary(aov(x12~grp,data=HBAT))
summary(aov(x15~grp,data=HBAT))
summary(aov(x18~grp,data=HBAT))
```
The ANOVA shows, that all variables except x18 Delivery speed have HIGHLY significant difference within the mean of the groups.

 o X6: Product Quality
 - eg. x6 has means which differs in the group, as this group may be more volatile due to the scores given in each observation in the group
 - Mean Sq: 38.50   

 o x18 Delivery speed
 - eg. x18 has means which is quite close to each other, yeilding a score where the observations are quite similar.
 - Mean Sq: 0.2883   



Snake plot
```{r Complete - Dendrogram red borders }
# snake plot
matplot(t(NH.fit[[2]]),type="l")
```
 o X-axis: Represents the variables (x6, x8, x12, x15, x18). The indices 1 to 5 correspond to these variables.

 o Y-axis: Represents the mean values of the variables for each cluster.
- on x-axis: 1 = x6 is overall scoring highest in the plot
- on x-axis: 5 = x18 is overall scoring lowest in the plot



I DONT KNOW WHY WE ASSESS THESE ANOVA values for x19","x20","x21","x22???
```{r Complete - Dendrogram red borders }
# criterion validity
aggregate(HBAT[,c("x19","x20","x21","x22")],list(grp),mean)
summary(aov(x19~grp,data=HBAT))
summary(aov(x20~grp,data=HBAT))
summary(aov(x21~grp,data=HBAT))
summary(aov(x22~grp,data=HBAT))
```



 o Profiling
-	The profiling is carried out by looking at the centroids of the cluster
-	The idea is to look for characteristics on one or several of the clustering variables that identify a cluster
-	In this way one can name each of the clusters based on the information just identified
-	The profiling may also be helpful in choosing from different potential cluster solutions

Categorical variables distribution
 - demonstrate the use of Chi-Squared tests to examine the association between different categorical variables and the clusters obtained from k-means clustering.
```{r Profiling - Chisquare association between categorical variables }
# profiling
tbl <- table(HBAT$x1,grp)
round(100*prop.table(tbl,2))
chisq.test(tbl)

tbl <- table(HBAT$x2,grp)
round(100*prop.table(tbl,2))
chisq.test(tbl)

tbl <- table(HBAT$x3,grp)
round(100*prop.table(tbl,2))
chisq.test(tbl)

tbl <- table(HBAT$x4,grp)
round(100*prop.table(tbl,2))
chisq.test(tbl)

tbl <- table(HBAT$x5,grp)
round(100*prop.table(tbl,2))
chisq.test(tbl)
```
x1: less than 1 year, 1 to 5 years, over 5 years
                grp
                    1  2  3  4
  less than 1 year 29 18 46 26
  1 to 5 years     35 18 54 32
  over 5 years     35 64  0 42

X-squared = 24.074, df = 6, p-value = 0.0005061

- x1 (length of time): There is a significant association between the clusters and the length of time categories (less than 1 year, 1 to 5 years, over 5 years). The distribution of these categories varies significantly across the clusters.

x3:(company size)
                  grp
                    1  2  3  4
  small (0 to 499) 41 64 39 48
  large (500+)     59 36 61 52

X-squared = 3.326, df = 3, p-value = 0.344

- x3 (company size): There is NOT a significant association between the clusters and the company size categories (small, large). The distribution of these categories has statistically similar behavior across the clusters.



### Toy Example which is STUPID

```{r  }
var1 <- c(5,6,15,16,25) 
var2 <- c(5,6,14,15,20)
toy <- data.frame(var1,var2)
toy.dist <- dist(toy,method="euclidean") #toy.dist computes the Euclidean distance matrix for the data in toy.
```


```{r  }
toy.dist2 <- toy.dist^2 #toy.dist2 squares the distances, which is not a typical step for hierarchical clustering but may be done here for illustrative purposes.
```


```{r  }
toy.H.single <- hclust(toy.dist2,method="single") #toy.H.single performs hierarchical clustering using the single linkage method on the squared distance matrix
```


```{r  }
# the actual amalgamation (sammenlægning)
toy.H.single$merge
```
 - merge component of the hierarchical clustering object shows how clusters are merged at each step of the algorithm. The merge matrix has two columns, where each row represents a merge step:
Negative values indicate merging an individual data point.
Positive values indicate merging previously formed clusters.
SUMMARY
 - The code provides an example of how to perform hierarchical clustering using single linkage on a small dataset. The merge matrix helps to understand the sequence in which data points or clusters are combined at each step. The output suggests an iterative clustering process, where initially individual points are merged, followed by merging the resulting clusters.
 
how to extract the heights (or distances) at which clusters are merged in a hierarchical clustering analysis using the single linkage method.
```{r  }
# the associated increase in distance/heterogeneity
toy.H.single$height
```


```{r  }
# a permutation of the original observations suitable for plotting
toy.H.single$order
```


```{r  }
# plot
plot(toy.H.single)
```


```{r  }
# other linkage possibilities
toy.H.complete <- hclust(toy.dist,method="complete")
toy.H.average <- hclust(toy.dist,method="average")
toy.H.centroid <- hclust(toy.dist,method="centroid")

# Wards method
toy.H.ward <- hclust(toy.dist2,method="ward.D")

## Distance vs shape
scores<-matrix(c(21,34,17,42,62,75,58,85),nrow=4,byrow=F)
matplot(scores,type="l")
dist(t(scores),method="euclidean")
cor(scores)
```
Plot:
The plot helps in visualizing the score patterns across different categories, showing the relationship between them. The distance and correlation calculations provide quantitative measures of similarity and dissimilarity between the score patterns. This kind of analysis is useful for understanding how different data points or clusters compare in terms of their attributes or features.

Output
Distance Calculation:
 - Euclidean distance (83.01807) suggests a significant difference in the absolute values of the scores between the two patterns.

Correlation Matrix:
 - Despite the large Euclidean distance, the high correlation (0.9987087) indicates that the two score patterns have a similar shape or trend. They rise and fall in almost the same manner, which is why the correlation is so high.



#II Segmentation

## Model-Based Clustering - Statistical model

EXACT IDENTICAL TO SEGMENTATION 1 INTRO!!!

```{r 20.segmentation}
# R script for the analysis of the HBAT example in the model-based clustering
# lecture
library(readstata13)
rm(list=ls())

HBAT <- read.dta13("Datasets/hbat.dta")
```

first we make our own id variable based on the original order
```{r 20.segmentation}
nobs <- nrow(HBAT)

HBAT$id <- seq(1,nobs)
# pick the variables for cluster analysis and summarize them
hbat <- c("x6","x8","x12","x15","x18")
summary(HBAT[,hbat])
```
X6: Product Quality
X8: Technical Support
X12: Salesforce Image
X15: New Products
X18: Delivery Speed

EXACT IDENTICAL TO SEGMENTATION 1 INTRO!!!

```{r SD and Multicollinearity}
apply(HBAT[,hbat],2,sd)

# multicollinearity should not be a problem
cor(HBAT[,hbat])
# which is confirmed
```
 o Standard Deviation
x6        x8       x12       x15       x18 
1.3962793 1.5304568 1.0723198 1.4930478 0.7344372
 - standard deviations provide insights into the variability of each column.
 - x8 Technical Support: 1.5304568 The highest variance in customer experience, which states that the technical capabilities is the most varied compentence within these Business areas

 o Multicollinearity
             x6          x8         x12         x15
x6   1.00000000  0.09560045 -0.15181285  0.02698816
x8   0.09560045  1.00000000  0.01699055 -0.07357899
x12 -0.15181285  0.01699055  1.00000000  0.03164010
x15  0.02698816 -0.07357899  0.03164010  1.00000000
x18  0.02771801  0.02544069  0.27155127  0.10574950
           x18
x6  0.02771801
x8  0.02544069
x12 0.27155127
x15 0.10574950
x18 1.0000000
- Values above 0.7 or 0.8 (or below -0.7 or -0.8) are often considered high, indicating potential multicollinearity problems.
Values between 0.5 and 0.7 (or -0.5 and -0.7) suggest moderate correlation, but they might still be acceptable depending on the analysis.


EXACT IDENTICAL TO SEGMENTATION 1 INTRO!!!

Outlier detection
o	An object which doesn’t fit into a given pattern may represent: 
 - a true outlier, a small but insignificant segment of the population or 
  - an actual and relevant group which is underrepresented in the sample
```{r Outlier detection}
# look for outliers
dev <- t(t(HBAT[,hbat])-apply(HBAT[,hbat],2,mean))
dev2 <- dev^2
sumdev2 <- rowSums(dev2)
sort(sqrt(sumdev2))
# observations 6 and 87 are candidates for deletion
```
The outliers are assessed in the relationship of the outliers being next to or before in the sequence.
The distance between obs 90 and 6 is 0.68, which is a value giving the analyst the possibility to extract obs 6 from the dataset.

 o Acceptable distance: 0.42
Observation 44 : 4.051562   
Observation 53  : 4.475840   
Obs 44, 53 distance: 4.475840 - 4.051562 = 0.42

 o Deletion distance: 0.68
Observation 90 : 4.620276 
Observation 6  : 5.304446 
Obs 90, 6 distance: 5.304446 - 4.620276 = 0.68

 o Deletion distance: 0.96
Observation 90 : 4.620276 
Observation 87  : 5.575872  
Obs 90, 6 distance: 5.575872  - 4.620276 = 0.96

SUMMARY: Observation 6 and 87 are deleted on the basis of the calculations^

EXACT IDENTICAL TO SEGMENTATION 1 INTRO!!!


Extracting variable 6 and 87
```{r Delete variables}
HBAT <- subset(HBAT,id!="6"&id!="87") #extracting variable 6 and 87
```
EXACT IDENTICAL TO SEGMENTATION 1 INTRO!!!


Rearranging dataframe and rows
```{r Rearranging dataframe }
nobs <- nrow(HBAT) #number of rows in the dataset
HBAT$id <- seq(1,nobs) #number of observations (nobs) is recalculated, and the id column is reinitialized to have a sequential order starting from 1 up to the number of observations.
```
EXACT IDENTICAL TO SEGMENTATION 1 INTRO!!!


-	Model-based clustering assumes that the population is made up of several distinct subsets/clusters, each governed by a different multivariate probability density function
-	The parameters associated with the model can be used to assign each observation a posterior probability of belonging to a cluster
-	The problems of identifying the number of clusters and selecting the clustering method boils down to a model selection problem – for which we have a number of procedures

-	Furthermore, being based on a genuine statistical model, model-based clustering readily accommodates missing data in a way similar to the state-of-the-art ML methods
-	Are also known as latent-class cluster analysis or finite mixture modeling
-	K-means clustering are approximate estimation methods for certain finite mixture probability models lending credibility to these
-	They allow for an integral representation of the cluster model together with predictor variables such as demographics

BIC decides the number of clusters
```{r Model-based clustering }
library("mclust")
(mc <- Mclust(HBAT[,hbat])) #Mclust function performs model-based clustering on the data and selects the best model based on the Bayesian Information Criterion (BIC).
```
Model-based clustering fits a mixture model to the data and selects the optimal model and number of clusters using statistical criteria.

(EEI,3) 
 - indicates that the best model is of type "EEI" with 3 clusters.
 - EEI: Equal volume, equal shape, and variable orientation. 
 - means that the clusters are assumed to have equal volume and shape, but their orientation may vary.


BIC (Bayesian Information Criterion) plot generated by the mclust package in R.
 - shows BIC values for different models and numbers of components (clusters).
 - Choose cluster solution on the basis of above EEI
```{r BIC Plot}
# Assess different solutions
plot(mc, HBAT[,hbat], what = "BIC", col = "black")
```
 o X-axis: number of components (clusters) ranging from 1 to 9.

 o Y-axis: BIC, Lower BIC values indicate a better model fit.

 - BIC plot is a critical tool in model-based clustering as it provides a visual representation of how different models perform across various numbers of clusters. 
 - The model with the lowest BIC value for a given number of clusters is typically selected as the best fit. 
 - This approach helps in objectively determining the optimal number of clusters and the most appropriate model for the data, balancing model complexity and goodness of fit. 
 
 SUMMARY:
In this case, the EEI model with 3 clusters was selected based on its BIC value.
THIS CASE decides on the basis on BIC closest to zero!
= EEI Cluster: 3



mc: Model Clustering
```{r 20.segmentation}
# Summarize
summary(mc)
```
Output 1:
Clustering table:
 1  2  3 
17 59 22 

Output 2:
Log-Likelihood
 - log-likelihood of the fitted model. The log-likelihood is a measure of how well the model explains the data; higher values indicate a better fit. 
 Log-likelihood: -744.0524.

BIC
- Bayesian Information Criterion (BIC) is used for model selection. It balances model fit with model complexity, penalizing models with more parameters to prevent overfitting. Lower BIC values indicate better models. 
 - BIC: -1588.974

ICL
 - Integrated Complete Likelihood (ICL) criterion is another measure used for model selection, often in clustering. It incorporates both the BIC and an entropy term to account for the uncertainty in cluster assignments. Lower ICL values indicate better models. 
 - ICL: -1605.689


4 Cluster solution (rejects the EEIClust3 recommendation)
```{r 20.segmentation }
# Look at a 4-cluster solution
mc4 <- Mclust(HBAT[,hbat],G=4)
summary(mc4)
```
Output 1:
Clustering table:
 1  2  3  4 
22 34 16 26 

Output 2:
Log-Likelihood
 - log-likelihood of the fitted model. The log-likelihood is a measure of how well the model explains the data; higher values indicate a better fit. 
 - mc: Log-likelihood: -744.0524.
 - mc4: Log-likelihood: -740.7139
 - mc4 is the highest value

BIC
- Bayesian Information Criterion (BIC) is used for model selection. It balances model fit with model complexity, penalizing models with more parameters to prevent overfitting. Lower BIC values indicate better models. 
 - mc: BIC: -1588.974
 - mc4: BIC: -1609.807
 - mc4 is the lowest value

ICL
 - Integrated Complete Likelihood (ICL) criterion is another measure used for model selection, often in clustering. It incorporates both the BIC and an entropy term to account for the uncertainty in cluster assignments. Lower ICL values indicate better models. 
 - ICL: -1605.689
 - ICL: -1633.783
 - mc4 is the lowest value

SUMMARY: mc4 is performing best according to BIC, Log-Likelihood, ICL
 - Cluster 4 solution is the prefered!



#III Segmentation - Finding, assessing, predicting customer segments

## McD Example

```{r dataload McD}
# R script for the analysis of the McDonald's example in the segmentation lecture
#install.packages("flexclust")
#install.packages("partykit")
#install.packages("clue") #- should be installed as part of the flexclust but isn't
library("flexclust")
library("partykit")
library("clue")
rm(list=ls())

## Preparation - Read in data
library(readr)
mcdonalds <- read_csv("Datasets/mcdonalds.csv")
```


```{r data insights}
# Look into content
#glimpse(mcdonalds)
str(mcdonalds)
head(mcdonalds, 3) #showing first 3 rows in dataset mcdonalds
```


```{r Discretize Yes:1 NO:0 }
# Change Yes/No to 1/0
MD.x <- as.matrix(mcdonalds[, 1:11]) #function converts this selected subset of the data frame into a matrix called MD.x. mcdonalds[, 1:11] selects all rows and the first 11 columns of the mcdonalds data frame.
MD.x <- (MD.x == "Yes") + 0 # TRUE can be coerced to 1 and FALSE to 0. Therefore, adding 0 (+ 0) to the logical matrix effectively converts all TRUE values to 1s and all FALSE values to 0s.
```


```{r Column means }
## Exploring data
# What fraction sees McDonald's possessing each of the attributes
round(colMeans(MD.x), 2) #rounding the column means to two decimals
```
Column Means:
yummy convenient      spicy  fattening     greasy 
0.55       0.91       0.09       0.87       0.53
      
fast      cheap      tasty  expensive    healthy  disgusting
0.90       0.60       0.64       0.36       0.20    0.24



```{r PCA}
# PCA and perceptual map
MD.pca <- prcomp(MD.x)
summary(MD.pca)
```
 o PCA Principal Component analysis:
- The idea behind PCA is to reduce the dimensionality of the dataset by transforming the original variables into a new set of variables (the principal components) which are uncorrelated, and which are ordered so that the first few retain most of the variation present in all of the original variables.
- The greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.

 o Standard Deviation:
- This indicates how much variance is captured by each principal component. A higher standard deviation means that component accounts for a larger variance within the dataset.

 o Proportion of Variance:
- squared standard deviation for each component divided by the sum of squared standard deviations of all components. It tells us the proportion of the dataset’s total variance that is captured by each principal component.



```{r Rotation/Loadings PCA}
print(MD.pca, digits = 2)
```
Rotation matrix:
- Each row corresponds to one of the original variables (attributes like "yummy", "convenient", "spicy", etc.), and each column corresponds to a principal component (PC1 to PC11). The values in the matrix show how each attribute contributes to each principal component.

Example (original variable yummy on PC1):
- The attribute "yummy" has a strong positive loading on PC1 (0.4769) and a strong negative loading on PC9 (-0.5724), which means "yummy" is an important attribute that defines the direction of PC1 and PC9.


```{r PCA loading-plot}
plot(predict(MD.pca), col = "grey")
projAxes(MD.pca)
```
- dots represent the data points projected onto the first two principal components, PC1 and PC2. This provides a two-dimensional summary of the data, where similar data points are clustered together, and different ones are farther apart.
- arrows (or vectors) represent the loadings of each attribute (like "yummy", "cheap", "expensive", etc.), which point in the direction of the maximum variance for that attribute. The length of an arrow indicates the strength of the attribute's contribution to the two principal components displayed.
- Attributes that are close together are positively correlated, while those that are on opposite ends of the plot are negatively correlated.
- Attributes that are close to the origin (center of the plot) have smaller effects on the variation explained by the first two principal components, while those further out are more influential.

It is seen that "tasty" and "yummy" point in a similar direction, suggesting they contribute similarly to the dataset's variance and are likely correlated. Conversely, "expensive" points in a roughly opposite direction to "cheap", indicating they are negatively correlated. The plot also suggests that attributes like "disgusting" and "fattening" may not be strongly related to the primary variance within the data, as their vectors are shorter and closer to the center of the plot.


Extracting segments
```{r k-means Within-cluster distances Plot }
# k-means
set.seed(28)
MD.km28 <- stepFlexclust(MD.x, 2:8, nrep = 10,verbose = FALSE) #compute k-means clustering on the matrix MD.x. The function is likely a stepwise routine that evaluates the optimal number of clusters ranging from 2 to 8.
#nrep = 10 indicates that each k-means operation is repeated 10 times to avoid local minima. 
#verbose = FALSE suppresses the printing of additional output during the computations

plot(MD.km28, xlab = "number of segments")
```
Plot: 
- bar chart that displays the sum of within-cluster distances for each number of clusters tested (from 2 to 8). Ideally, one looks for a "elbow" in the plot where the rate of decrease sharply changes, suggesting a natural grouping in the data. This "elbow" is often considered as an indicator of the optimal number of clusters to use.



Rand Index
 - Rand  index evaluates the similarity between the clustering results across multiple bootstrap samples. 
 - The higher the adjusted Rand index, the more stable the clustering solution is.
 - Bootstrapping method for evaluating k-means clustering along with a boxplot visualizing the results
```{r Adj Rand index - bootstrap evaluating k-means}
# Segment stability within the same number of segments
#long computation time!
set.seed(1234)
MD.b28 <- bootFlexclust(MD.x, 2:8,
                        nrep = 10, #nrep = 10 parameter indicates that the clustering should be repeated 10 times for stability, 
                        nboot = 100) #a bootstrapping method for k-means clustering on the dataset MD.x. nboot = 100 indicates that 100 bootstrap resamples should be used to assess the stability of the clustering solution.

plot(MD.b28, xlab = "number of segments",ylab = "adjusted Rand index")
```
 o Boxplot 
- visualizes the distribution of the adjusted Rand index for each number of segments (2 through 8). 
- This plot can help determine the number of clusters that yield the most stable and consistent partitioning of the data by looking for the number of segments that consistently show high adjusted Rand index values
SUMMARY: 
 - from this boxplot, a 4-cluster solution is evaluated as the most optimal amount of clusters



Gorge plots -	For well-separated segments there should mainly be many low and many high similarity measures – hence the name (gorge/kløft)
Morten: - the gorge plot indicates that none of the segments in the four-segment solution are very well separated
```{r Gorge plot}
MD.km28 <- relabel(MD.km28) #apply the reLabel function to the clustering object MD.km28. This function is typically used to reorder the labels of the clusters for clarity or based on some criteria, although the specific behavior depends on the implementation within the package being used

histogram(MD.km28[["4"]], data = MD.x, xlim = 0:1) #using 4 clusters in this specific example
#xlim = 0:1 sets the limits for the x-axis from 0 to 1, assuming the similarity scores are normalized to this range.
```
Morten: - the gorge plot indicates that none of the segments in the four-segment solution are very well separated.

 o Gorge plots (histograms):
- These histograms can help in interpreting the structure of the clusters. 
 - For example, a cluster with many high similarity scores (bars towards the right of the histogram) contains data points that are very similar to each other, indicating a tight and well-defined cluster.
 - Conversely, a cluster with scores spread across the range might be less well-defined or could contain subgroups.
- the y-axis is labeled "Percent of Total", indicating that the histograms show the distribution of the similarity scores as a percentage of the total number of scores within each cluster. 
- the x-axis is labeled "similarity", denoting the range of similarity scores from 0 to 1.



SLS_A plot - Segment Level Stability Across solutions - plot
```{r SLS_A plot - Segment Level Stability Across solutions}
# Segment level stability across solutions
slsaplot(MD.km28)
```
Goes from 2 cluster -> 3 cluster -> 4 cluster -> and so on...
SLS_A plot:
- The links (or bands) between the nodes show how many data points from each cluster in one solution correspond to clusters in another solution. In this case, it likely shows how segments in a lower number of clusters (on the left) are combined or split as the number of clusters increases (moving to the right).
- thickness of the bands is proportional to the number of data points that follow that path, illustrating the size of the clusters and the movement of data points between them. A stable clustering will have stronger, more direct paths, while a less stable one will show a more complex network of paths, indicating that the data points are being reassigned to different clusters across solutions.
- This visualization is valuable for assessing the robustness of clustering. If many data points maintain their cluster membership across different solutions, it indicates that the clustering is stable and likely meaningful. If there is a lot of reassignment across solutions, it suggests that the cluster structure is not as clear-cut.



Specific X-cluster(4)
```{r Specific X-cluster(4) - }
# Save the four-segment solution
MD.k4 <- MD.km28[["4"]] #defining the amount of clusters
MD.r4 <- slswFlexclust(MD.x, MD.k4) #function slsWFlexclust is applied to the dataset MD.x and the four-cluster solution MD.k4.

plot(MD.r4, ylim = 0:1, xlab = "segment number",ylab = "segment stability")
```
Specific X(4)-cluster boxplot:
- boxplot shows the distribution of stability values for each of the X(4) segments. 
 - In a boxplot, the box represents the interquartile range (IQR), which contains the middle 50% of the data. The line inside the box represents the median value. The whiskers represent the range of the data within 1.5 times the IQR above and below the box. Points outside this range are considered outliers and are plotted as individual points.
e.g.:
- Segment 1 and 2 seems to have lower median stability and higher variability than the other segments, as shown by the narrow box and the presence of outliers.
- Segment 4 and 3 shows a higher median stability and less variability in stability, as indicated by the wider box.



Segment Profile Plot - Profiling segments
 - The segment profile plot makes it easy to see key characteristics of each segment and highlights differences between segments
```{r Segment Profile Plot - Profiling segments}
MD.vclust <- hclust(dist(t(MD.x))) #performs hierarchical clustering on the transpose of the matrix MD.x. Hierarchical clustering is a method of cluster analysis which seeks to build a hierarchy of clusters. The dist function calculates the distance matrix between the rows of MD.x (which become columns after the transpose operation), and hclust is then used to create a hierarchical clustering from that distance matrix. The result is stored in MD.vclust.

barchart(MD.k4, shade = TRUE,which = rev(MD.vclust$order)) #generates a barchart using the object MD.k4, which likely contains the clustering output from a previous k-means analysis. 
#shade = TRUE argument adds shading to the bars to give a visual indication of significance or magnitude. 
#which = rev(MD.vclust$order) argument reorders the barchart to match the order of clusters determined by the hierarchical clustering object MD.vclust. This reordering is done in reverse (rev) to perhaps match a specific ordering convention or for better visual clarity.
```
Segment profile plot:
- a set of barcharts that profile attributes across four different clusters. For each cluster, the attributes are listed along the y-axis, and the x-axis represents some measure related to the attribute, typically its frequency or importance within the cluster.
- each bar might represent the mean value of the attribute within the cluster, and the red dot could indicate the median value or a special data point like an outlier.
- the numbers at the top of each barchart (e.g., Cluster 1: 499 (34%)) likely indicate the number of observations within each cluster and the percentage of the total dataset that each cluster represents.
SUMMARY MORTEN:
o	The number of consumers in each segment varies – segment 1 is the largest, segment 2 is the smallest
o	Segment 1 sees McDonald’s as cheap (and not particularly healthy) – this is unique for segment 1
o	Segment 2 sees McDonald’s as expensive and disgusting – this combination is specific to segment 2
o	Segment 3 also sees McDonald’s as expensive but also as tasty and yummy
o	Segment 4 sees McDonald’s as cheap, but also healthy, tasty, and yummy


Segmentation separation plot: 
 - represent the solution by adding the four centroids to our perceptual map and change the colouring to separate the observations from different segments.
- scatterplot for visualizing the separation of segments (or clusters) that were identified in a dataset, overlaid with the loadings of the attributes on the first two principal components from a PCA analysis.
```{r # Segment separation plot }
plot(MD.k4, project = MD.pca, data = MD.x,hull = FALSE, simlines = FALSE,
     xlab = "principal component 1",ylab = "principal component 2") #generates a scatter plot of the clusters defined in MD.k4, projecting the data points onto the first two principal components from MD.pca. 
#data = MD.x argument indicates the dataset used for the PCA. 
#hull = FALSE parameter suggests that convex hulls are not drawn around the clusters
#simlines = FALSE means that similarity lines (often used to connect similar items) are not drawn.

projAxes(MD.pca) # function, projAxes, is typically used to overlay the principal component loadings onto the plot. In this context, it probably adds the vectors that represent the directions and magnitudes of the original variables (attributes like "tasty", "cheap", "expensive", etc.) in the space defined by the first two principal components.
```
Segmentation separation plot:
- shows the data points from the dataset, with each point representing a single observation and the shape/color likely corresponding to the cluster to which the observation has been assigned.
- includes vectors that represent how each attribute aligns with the principal components. For instance, an attribute pointing towards the right on the x-axis (principal component 1) has a strong positive loading on that component, whereas an attribute pointing upwards on the y-axis (principal component 2) has a strong positive loading on the second component.
- helps to interpret the PCA by showing which attributes are most characteristic of each principal component, and also demonstrates the relationships between the clusters and the attributes.


Mosaic plot - Describing segments:
- extracting membership of segments for each consumer and creating a mosaic plot to describe the segments in terms of consumer opinions
```{r Mosaic plot LIKE - Describing segments }
# Extract the segment membership for each consumer
mcdonalds$k4 <- clusters(MD.k4) # extracts the cluster membership for each data point from the MD.k4 object, which presumably contains a k-means clustering result. The cluster membership for each data point (each consumer) is then stored in a new column $k4 of the mcdonalds data frame.

# Prepare the like/hate variable
mcdonalds$like <- factor(mcdonalds$Like,levels=c("I love it!+5","+4","+3","+2","+1","0","-1","-2","-3","-4","I hate it!-5")) #converts the like column of the mcdonalds data frame into a factor with ordered levels. These levels represent a scale of preference, from "I love it!+5" to "I hate it!-5", suggesting that there is an original numerical scale of -5 to +5, where +5 signifies strong liking and -5 signifies strong disliking.

# Make mosaic plot of like/hate
mosaicplot(table(mcdonalds$k4, mcdonalds$like), shade = TRUE,main = "", 
           xlab = "segment number") #mosaic plot is created from the cross-tabulation of mcdonalds$k4 and mcdonalds$like. The shade = TRUE argument indicates that the cells of the mosaic plot will be shaded according to the standardized residuals, which highlight the cells that have higher or lower counts than expected under independence.
```
Mosaic plot - Describing segments:
o	The length of each mosaic is proportional to the size of the segment
o	The height of each mosaic is proportional to the number of respondents in the category
o	The color indicates the differences between observed and expected (under the independence model) number of respondents 
	red = fewer observed respondents than expected 
	blue = more observed respondents than expected
SUMMARY MORTEN:
o	Members of segment 1 dislike (rarely loves) McDonald’s
o	However, members of segment 4 are much more prone to loving and less likely to hate McDonald’s
o	Members of segment 2 are those with the strongest negative feelings toward McDonald’s
o	Segments 1 and 3 have the same gender distribution as the overall sample
o	Segment 2 has more males and fewer females whereas the opposite patterns characterizes segment 4



```{r Gender - Boxplot + Mosaic plot}
mosaicplot(table(mcdonalds$k4, mcdonalds$Gender), shade = TRUE)

boxplot(mcdonalds$Age ~ mcdonalds$k4,varwidth = TRUE, notch = TRUE) # formula indicates that the boxplot should compare the age variable (likely the age of the customers) across the different clusters specified in mcdonalds$k4.
#varwidth = TRUE: This argument specifies that the width of the boxplots should be proportional to the square root of the number of observations in the cluster. This helps visualize the relative sizes of the clusters.
#notch = TRUE: This argument adds notches to the boxplots. Notches are useful for assessing the significance of the difference between medians; if the notches of two boxes do not overlap, this suggests that the medians are significantly different.
```
Gender mosaic plot:
- same structure as "describing segments - mosaic plot"
- Distribution of gender in each of the segments/clusters

Gender Box-plot
- y-axis represents the ages of the respondents.
- the variation in box widths suggests that the number of respondents varies across the clusters. 
- the presence of notches around the medians indicates where the medians lie and whether they might be significantly different from one another.
SUMMARY MORTEN
o	Members of segment 3 (those seeing McDonald’s as tasty and yummy) are younger than the other segments


Predict segment 3 membership using a classification tree

```{r Predict segment 3 membership using a classification tree }
# Prepare visit frequency variable
mcdonalds$visitfrequency <- factor(
  mcdonalds$VisitFrequency,
  levels=c("Never","Once a year","Every three months","Once a month","Once a week","More than once a week"))
tree <- ctree(factor(k4==3) ~ #change this due the specific segment
                like+Age+visitfrequency+factor(Gender),
              data=mcdonalds)
plot(tree) #interpret
```
 o Node 1: 
 - The first split is based on the like variable (with p < 0.001). This variable measures the degree of liking for McDonald's on a scale from -5 (I hate it) to +5 (I love it).

 o Node 2: 
 - For customers with a positive liking (like > 0), the next split is based on Age (p < 0.001).

 o Node 3: 
 - For customers aged 47 or less, the next split is based on Gender (p = 0.007).
 Terminal node (4, 5, 6, 9, 10, 11) 
 - has a bar plot showing the proportion of TRUE (segment 3 members) and FALSE (non-segment 3 members) cases. 
 - For example: Node 4: The bar plot shows the distribution of female customers aged 47 or less.


MORTEN: -	Based on the knock-out criteria and the segment attractiveness criteria we can develop a segment evaluation plot
 - "knock-out criteria" refers to specific conditions or standards used to eliminate certain segments from consideration during the process of selecting target segments. These criteria act as a filter to ensure that only segments meeting minimum requirements are considered for further analysis and targeting. 
 - The x-axis holds frequency of visiting McDonald’s
```{r Visit - Selecting target segment(s)}
visit <- tapply(as.numeric(mcdonalds$visitfrequency),mcdonalds$k4,mean) #calculates the mean visit frequency for each segment in the McDonald's dataset.
visit
```
"Visit" frequency is scored as 1=Never,...,6=More than once a week, not strictly aligned with the ratio properties hidden in the scale
Segment 1: Mean visit frequency is approximately 3.082.
Segment 2: Mean visit frequency is approximately 2.476.
Segment 3: Mean visit frequency is approximately 3.879.
Segment 4: Mean visit frequency is approximately 4.016

Morten: The y-axis holds the extent to which consumers love/hate McDonald’s
```{r Like - Selecting target segment(s) }
# Turn the current like scale upside down and recenter 
# (1=I love it,...,11=I hate it -> 5=I love it,...,-5=I hate it)
like <- tapply(-as.numeric(mcdonalds$like)+6,mcdonalds$k4,mean)
like
```
Segment 1: Mean LIKE frequency is approximately 0.002 (remember the scale from -5 to +5)
Segment 2: Mean LIKE frequency is approximately -2.483
Segment 3: Mean LIKE frequency is approximately 2.255
Segment 4: Mean LIKE frequency is approximately 2.887


```{r Gender - Selecting target segment(s) }
# Create the gender variable
female <- tapply((mcdonalds$Gender=="Female")+0,mcdonalds$k4,mean)
female
```
Segment 1: Proportion of females is approximately 0.583
Segment 2: Proportion of females is approximately 0.430
Segment 3: Proportion of females is approximately 0.469
Segment 4: Proportion of females is approximately 0.626

Segment evaluation  scatter 
 - evaluates segments based on 
 - visit frequency and 
 - liking scores, with the 
 - size of the bubbles representing the proportion of females in each segment.
```{r segment evaluation plot }
plot(visit, like, cex = 10 * female,xlim = c(2, 4.5), ylim = c(-3, 3))
text(visit, like, 1:4)
```
Segment 1:
 - Located around (3.1, 0.3).
 - Moderate visit frequency and moderate liking score.
 - The bubble size indicates a moderate proportion of females (~58.3%).

Segment 4:
 - Located around (4.0, 0.3).
 - Highest visit frequency and positive liking score.
 - The bubble size indicates the highest proportion of females (~62.6%).

MORTEN SUMMARY:
-	Each of the four P’s – Price, Product, Promotion, and Place will have to be adjusted according to the chosen target segment
-	If segment 3 is chosen, McDonald’s will have to cater to young customers with a favourable perception of McDonald’s who see it’s products as expensive, but also tasty and yummy
-	The authors suggest the MCSUPERBUDGET to address the Price dimension with appropriate adjustments to the remaining P’s
-	The success of the chosen strategy must be evaluated and the market continuously monitored


#IV Segmentation - Latent Class Analysis

## McD Example
o	The relative position of latent class models and measures of interrelationships for categorical (binary) data
o	The latent class model for binary data and estimation of such models
o	Assessment and allocation to classes for such a model

EXACT SAME AS III SEGMENTATION
```{r dataload McDonalds}
# R script for the analysis of the McDonald's example in LCA I lecture
library("flexclust")
library("flexmix")
rm(list=ls())
# Read in data
mcdonalds <- read_csv("Datasets/mcdonalds.csv")
```

EXACT SAME AS III SEGMENTATION
```{r data insights}
# Look into content
names(mcdonalds)
dim(mcdonalds)
head(mcdonalds, 3)
```

EXACT SAME AS III SEGMENTATION
```{r Discretize Yes:1 NO:0 }
# Change Yes/No to 1/0
MD.x <- as.matrix(mcdonalds[, 1:11]) #function converts this selected subset of the data frame into a matrix called MD.x. mcdonalds[, 1:11] selects all rows and the first 11 columns of the mcdonalds data frame.
MD.x <- (MD.x == "Yes") + 0 # TRUE can be coerced to 1 and FALSE to 0. Therefore, adding 0 (+ 0) to the logical matrix effectively converts all TRUE values to 1s and all FALSE values to 0s.
head(MD.x, 3)
```

EXACT SAME AS III SEGMENTATION
```{r Column means }
## Exploring data
# What fraction sees McDonald's possessing each of the attributes
round(colMeans(MD.x), 2) #rounding the column means to two decimals
```
Column Means:
yummy convenient      spicy  fattening     greasy 
0.55       0.91       0.09       0.87       0.53
      
fast      cheap      tasty  expensive    healthy  disgusting
0.90       0.60       0.64       0.36       0.20    0.24

EXACT SAME AS III SEGMENTATION
```{r PCA}
# PCA and perceptual map
MD.pca <- prcomp(MD.x)
summary(MD.pca)
```
PCA Principal Component analysis:
- The idea behind PCA is to reduce the dimensionality of the dataset by transforming the original variables into a new set of variables (the principal components) which are uncorrelated, and which are ordered so that the first few retain most of the variation present in all of the original variables.
- The greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.
Standard Deviation:
- This indicates how much variance is captured by each principal component. A higher standard deviation means that component accounts for a larger variance within the dataset.
Proportion of Variance:
- squared standard deviation for each component divided by the sum of squared standard deviations of all components. It tells us the proportion of the dataset’s total variance that is captured by each principal component.

EXACT SAME AS III SEGMENTATION
```{r Rotation/Loadings PCA}
print(MD.pca, digits = 2)
```
Rotation matrix:
- Each row corresponds to one of the original variables (attributes like "yummy", "convenient", "spicy", etc.), and each column corresponds to a principal component (PC1 to PC11). The values in the matrix show how each attribute contributes to each principal component.
Example (original variable yummy on PC1):
- The attribute "yummy" has a strong positive loading on PC1 (0.4769) and a strong negative loading on PC9 (-0.5724), which means "yummy" is an important attribute that defines the direction of PC1 and PC9.

EXACT SAME AS III SEGMENTATION
```{r PCA loading-plot}
plot(predict(MD.pca), col = "grey")
projAxes(MD.pca)
```
- dots represent the data points projected onto the first two principal components, PC1 and PC2. This provides a two-dimensional summary of the data, where similar data points are clustered together, and different ones are farther apart.
- arrows (or vectors) represent the loadings of each attribute (like "yummy", "cheap", "expensive", etc.), which point in the direction of the maximum variance for that attribute. The length of an arrow indicates the strength of the attribute's contribution to the two principal components displayed.
- Attributes that are close together are positively correlated, while those that are on opposite ends of the plot are negatively correlated.
- Attributes that are close to the origin (center of the plot) have smaller effects on the variation explained by the first two principal components, while those further out are more influential.

It is seen that "tasty" and "yummy" point in a similar direction, suggesting they contribute similarly to the dataset's variance and are likely correlated. Conversely, "expensive" points in a roughly opposite direction to "cheap", indicating they are negatively correlated. The plot also suggests that attributes like "disgusting" and "fattening" may not be strongly related to the primary variance within the data, as their vectors are shorter and closer to the center of the plot.

EXACT SAME AS III SEGMENTATION
Extracting segments
```{r k-means Within-cluster distances Plot }
# k-means
set.seed(28)
MD.km28 <- stepFlexclust(MD.x, 2:8, nrep = 10,verbose = FALSE) #compute k-means clustering on the matrix MD.x. The function is likely a stepwise routine that evaluates the optimal number of clusters ranging from 2 to 8.
#nrep = 10 indicates that each k-means operation is repeated 10 times to avoid local minima. 
#verbose = FALSE suppresses the printing of additional output during the computations

plot(MD.km28, xlab = "number of segments")
```
Plot: 
- bar chart that displays the sum of within-cluster distances for each number of clusters tested (from 2 to 8). Ideally, one looks for a "elbow" in the plot where the rate of decrease sharply changes, suggesting a natural grouping in the data. This "elbow" is often considered as an indicator of the optimal number of clusters to use.

EXACT SAME AS III SEGMENTATION
Rand Index
 - Rand  index evaluates the similarity between the clustering results across multiple bootstrap samples. 
 - The higher the adjusted Rand index, the more stable the clustering solution is.
 - Bootstrapping method for evaluating k-means clustering along with a boxplot visualizing the results
```{r Adj Rand index - bootstrap evaluating k-means}
# Segment stability within the same number of segments
#long computation time!
set.seed(1234)
MD.b28 <- bootFlexclust(MD.x, 2:8, nrep = 10,nboot = 100) #a bootstrapping method for k-means clustering on the dataset MD.x
#nrep = 10 parameter indicates that the clustering should be repeated 10 times for stability, '
#nboot = 100 indicates that 100 bootstrap resamples should be used to assess the stability of the clustering solution.

plot(MD.b28, xlab = "number of segments",ylab = "adjusted Rand index")
```
Boxplot 
- visualizes the distribution of the adjusted Rand index for each number of segments (2 through 8). 
- This plot can help determine the number of clusters that yield the most stable and consistent partitioning of the data by looking for the number of segments that consistently show high adjusted Rand index values
SUMMARY: 
 - from this boxplot, a 4-cluster solution is evaluated as the most optimal amount of clusters

EXACT SAME AS III SEGMENTATION
Gorge plots -	For well-separated segments there should mainly be many low and many high similarity measures – hence the name (gorge/kløft)
Morten: - the gorge plot indicates that none of the segments in the four-segment solution are very well separated
```{r Gorge plot}
MD.km28 <- relabel(MD.km28) #apply the reLabel function to the clustering object MD.km28. This function is typically used to reorder the labels of the clusters for clarity or based on some criteria, although the specific behavior depends on the implementation within the package being used

histogram(MD.km28[["4"]], data = MD.x, xlim = 0:1) #using 4 clusters in this specific example
#xlim = 0:1 sets the limits for the x-axis from 0 to 1, assuming the similarity scores are normalized to this range.
```
Morten: - the gorge plot indicates that none of the segments in the four-segment solution are very well separated.

Gorge plots (histograms):
- These histograms can help in interpreting the structure of the clusters. 
 - For example, a cluster with many high similarity scores (bars towards the right of the histogram) contains data points that are very similar to each other, indicating a tight and well-defined cluster.
 - Conversely, a cluster with scores spread across the range might be less well-defined or could contain subgroups.
- the y-axis is labeled "Percent of Total", indicating that the histograms show the distribution of the similarity scores as a percentage of the total number of scores within each cluster. 
- the x-axis is labeled "similarity", denoting the range of similarity scores from 0 to 1.

EXACT SAME AS III SEGMENTATION
SLS_A plot - Segment Level Stability Across solutions - plot
```{r SLS_A plot - Segment Level Stability Across solutions}
# Segment level stability across solutions
slsaplot(MD.km28)
```
Goes from 2 cluster -> 3 cluster -> 4 cluster
SLS_A plot:
- The links (or bands) between the nodes show how many data points from each cluster in one solution correspond to clusters in another solution. In this case, it likely shows how segments in a lower number of clusters (on the left) are combined or split as the number of clusters increases (moving to the right).
- thickness of the bands is proportional to the number of data points that follow that path, illustrating the size of the clusters and the movement of data points between them. A stable clustering will have stronger, more direct paths, while a less stable one will show a more complex network of paths, indicating that the data points are being reassigned to different clusters across solutions.
- This visualization is valuable for assessing the robustness of clustering. If many data points maintain their cluster membership across different solutions, it indicates that the clustering is stable and likely meaningful. If there is a lot of reassignment across solutions, it suggests that the cluster structure is not as clear-cut.


EXACT SAME AS III SEGMENTATION
Specific X-cluster(4)
```{r Specific X-cluster(4) - }
# Save the four-segment solution
MD.k4 <- MD.km28[["4"]] #defining the amount of clusters
MD.r4 <- slswFlexclust(MD.x, MD.k4) #function slsWFlexclust is applied to the dataset MD.x and the four-cluster solution MD.k4.

plot(MD.r4, ylim = 0:1, xlab = "segment number",ylab = "segment stability")
```
Specific X(4)-cluster boxplot:
- boxplot shows the distribution of stability values for each of the X(4) segments. 
 - In a boxplot, the box represents the interquartile range (IQR), which contains the middle 50% of the data. The line inside the box represents the median value. The whiskers represent the range of the data within 1.5 times the IQR above and below the box. Points outside this range are considered outliers and are plotted as individual points.
e.g.:
- Segment 1 and 2 seems to have lower median stability and higher variability than the other segments, as shown by the narrow box and the presence of outliers.
- Segment 4 and 3 shows a higher median stability and less variability in stability, as indicated by the wider box.
EXACT SAME AS III SEGMENTATION

FROM HERE MIXTURE MODELS///LATENT CLASS MODELS WHICH DIFFERS FROM^^^^

###Mixture models /// Latent Class models discussion
Latent Class Models (LCM)
 - Definition: Latent class models are a specific type of mixture model used primarily for categorical data, where the goal is to identify unobservable (latent) subgroups within the population based on observed categorical variables.
Mixture Models
 - Definition: Mixture models are a probabilistic model for representing the presence of subpopulations within an overall population without requiring that an observed data point be explicitly labeled as belonging to a particular subpopulation.
    Conclusion
 - In principle, latent class models can be viewed as a type of mixture model specifically tailored for categorical data. Both approaches share the common goal of uncovering hidden structure in the data through the use of latent variables.

Morten Latent Class Analysis
-	The general idea is still to assess whether the presence of some strong pairwise associations can be attributed to a common latent factor

Fit mixture model
Morten: 
-	In order to investigate the optimal number of segments we begin by estimating a mixture of binary distributions for all possible number of segments between two and eight
Morten:
-	Ten random restarts of the EM algorithm are used for each value of the number of clusters: nrep = 10
Morten:
- The decision regarding the number of segments is guided by the values of AIC, BIC, and ICL
```{r mixture distribution }
set.seed(1234)
MD.m28 <- stepFlexmix(MD.x ~ 1, k = 2:8, nrep = 10, model = FLXMCmvbinary(),verbose = FALSE)
MD.m28
```
iter: 
 - Number of iterations performed to reach convergence for each model.

k: 
 - Number of clusters (components) in the mixture model.

k0: 
 - Number of non-empty clusters after convergence.
 - Number of clusters including observations after convergence

logLik: Log-likelihood of the fitted model. 
 - log-likelihood is a measure of how well the model explains the data
 - Higher values indicate a better fit.

AIC Akaike Information Criterion. 
 - A lower AIC indicates a better model fit.
 - The model with 8 clusters (k=8) has the lowest AIC (13935.20) and BIC (14446.94), suggesting it might be the best model among those tried.

BIC Bayesian Information Criterion. 
 - is used for model selection. It balances model fit with model complexity, penalizing models with more parameters to prevent overfitting. 
 - Lower BIC values indicate better models
 - The model with 8 clusters (k=8) has the lowest AIC (13935.20) and BIC (14446.94), suggesting it might be the best model among those tried.

ICL Integrated Complete Likelihood criterion
 - another measure used for model selection, often in clustering. It incorporates both the BIC and an entropy term to account for the uncertainty in cluster assignments. Lower ICL values indicate better models. Here, the ICL is -1605.689.

SUMMARY:
 - output shows that the model with 8 clusters has the lowest AIC and BIC, suggesting it is the best fit according to these criteria. This information helps in understanding the underlying structure of the data and selecting an appropriate number of clusters for further analysis.
MORTEN SUMMARY
-	Adhering (fastholde) strictly to the information criteria BIC and ICL suggest a seven segment solution
-	AIC suggests an eight segment solution (which could, in principle, also be a nine or ten segment solution)
-	However, from the plot of the information criteria as a function of the number of segments not much is gained beyond a four segment solution


BIC AIC InfoCriteria Plot
```{r BIC AIC InfoCriteria Plot }
plot(MD.m28,ylab = "value of information criteria (AIC, BIC, ICL)")
```
Plot
 - shows that the model with 8 clusters has the lowest AIC and BIC, suggesting it is the best fit according to these criteria. This information helps in understanding the underlying structure of the data and selecting an appropriate number of clusters for further analysis.

MORTEN SUMMARY
-	Adhering (fastholde) strictly to the information criteria BIC and ICL suggest a seven segment solution
-	AIC suggests an eight segment solution (which could, in principle, also be a nine or ten segment solution)
-	However, from the plot of the information criteria as a function of the number of segments not much is gained beyond a four segment solution


MORTEN:
-	A cross table of the 4 segment solutions from the k-means and the mixture of distributions models can be used to assess the stability

Compares the cluster assignments from two different clustering methods:  - k-means and  
 - mixture model
 - followed by a contingency table showing the cross-tabulation from the k-means and mixturemodel
Choosing 4 clusters for mixture models in spite of BIC and AIC suggests 8...?
```{r k-means and mixturemodel }
MD.m4 <- getModel(MD.m28, which = "4") #retrieves the mixture model with 4 clusters CHOOSE BASED ON CRITERIA
table(kmeans = clusters(MD.k4),mixture = clusters(MD.m4)) 
#MD.k4 cluster assignments from k-means clustering stored 
#MD.m4 object now holds the mixture model with 4 clusters.
```
Columns (mixture): 
 - Represent the cluster assignments from the mixture model (1 to 4).
Rows (kmeans): 
 - Represent the cluster assignments from the k-means model (1 to 4).

Cluster 1 (kmeans) vs. Clusters (mixture):
245 observations 
 - assigned to cluster 1 by both k-means and the mixture model.
25 observations
 - assigned to cluster 1 by k-means and to cluster 2 by the mixture model.
228 observations 
 - assigned to cluster 1 by k-means and to cluster 3 by the mixture model.
1 observation 
 - is assigned to cluster 1 by k-means and to cluster 4 by the mixture model.
SUMMARY:
contingency table provides a detailed comparison of how observations are assigned to clusters by two different methods: k-means and the mixture model. This comparison helps in understanding the similarities and differences between the clustering results of the two methods. For example, we can see that clusters from the k-means method correspond somewhat differently to clusters from the mixture model, indicating different clustering structures identified by the two methods.


MORTEN:
-	The stable segments in the k-means solution (segments 2 and 3) are very similar to segments 4 and 2 in the finite mixture model solution
-	This is even more apparent (tydeligt) when we use the k-means solution as initialization values for the finite mixture model where segments 2,3, and 4 in the k-means solution are very similar to segments 2,3, and 4 in the finite mixture model
-	Notice that the log-likelihood values for the two mixture solutions are very similar suggesting that we have indeed found a global solution for the maximization problem

suggesting indeed found a global solution for the maximization problem

Fitting multivariate binary mixture model
fits a mixture model with predefined clusters from k-means and then compares the new cluster assignments with the original k-means clusters.
```{r Fitting multivariate binary mixture model }
MD.m4a <- flexmix(MD.x ~1, 
                  cluster = clusters(MD.k4),
                  model = FLXMCmvbinary()) #fitted model is a multivariate binary mixture model.
table(kmeans = clusters(MD.k4),mixture = clusters(MD.m4a))
#MD.k4 cluster assignments from k-means clustering stored 
#MD.m4a finite mixture model to the data MD.x using the cluster assignments from the k-means clustering stored in MD.k4
```
Columns (mixture): 
 - Represent the cluster assignments from the mixture model (1 to 4).
Rows (kmeans): 
 - Represent the cluster assignments from the k-means model (1 to 4).

Cluster 1 (kmeans) vs. Clusters (mixture):
269 observations 
 - assigned to cluster 1 by both k-means and the mixture model.
1 observation
 - assigned to cluster 1 by k-means and to cluster 2 by the mixture model.

25 observations 
 - assigned to cluster 1 by k-means and to cluster 3 by the mixture model.
204 observations 
 - assigned to cluster 1 by k-means and to cluster 4 by the mixture model.
SUMMARY: same as above



Log-likelihood
```{r }
logLik(MD.m4a)
logLik(MD.m4)
```
logLik: Log-likelihood of the fitted models. 
 - log-likelihood is a measure of how well the model explains the data
 - Higher values indicate a better fit.
LogLik - 7111.133 is the highest recommending the 
 - MD.m4 micture model which object holds the mixture model with 4 clusters.

Mixture models /// Latent Class models discussion
Latent Class Models (LCM)
 - Definition: Latent class models are a specific type of mixture model used primarily for categorical data, where the goal is to identify unobservable (latent) subgroups within the population based on observed categorical variables.
Mixture Models
 - Definition: Mixture models are a probabilistic model for representing the presence of subpopulations within an overall population without requiring that an observed data point be explicitly labeled as belonging to a particular subpopulation.
    Conclusion
 - In principle, latent class models can be viewed as a type of mixture model specifically tailored for categorical data. Both approaches share the common goal of uncovering hidden structure in the data through the use of latent variables.

MORTEN:
-	The stable segments in the k-means solution (segments 2 and 3) are very similar to segments 4 and 2 in the finite mixture model solution
-	This is even more apparent (tydeligt) when we use the k-means solution as initialization values for the finite mixture model where segments 2,3, and 4 in the k-means solution are very similar to segments 2,3, and 4 in the finite mixture model
-	Notice that the log-likelihood values for the two mixture solutions are very similar suggesting that we have indeed found a global solution for the maximization problem

suggesting indeed found a global solution for the maximization problem

WHICH NEEDS TO BE DONE FROM HERE:

Specific X-cluster(4)
```{r Specific X-cluster(4) - }
# Save the four-segment solution
MD.m4 <- MD.m28[["4"]] #retrieves the mixture model with 4 clusters CHOOSE BASED ON CRITERIA
MD.rm4 <- slswFlexclust(MD.x, MD.k4) #function slsWFlexclust is applied to the dataset MD.x and the four-cluster solution MD.m4.

plot(MD.rm4, ylim = 0:1, xlab = "segment number",ylab = "segment stability")
```
I HAVE TRIED TO CONVERT THIS CODE INTO A SUITABLE FORMAT - WITHOUT LUCK....
SHOW the findings from SEGMENTATION III INSTEAD

o	Profiling segments
```{r Segment Profile Plot - Profiling segments}

MD.vclust <- hclust(dist(t(MD.x))) #performs hierarchical clustering on the transpose of the matrix MD.x. Hierarchical clustering is a method of cluster analysis which seeks to build a hierarchy of clusters. The dist function calculates the distance matrix between the rows of MD.x (which become columns after the transpose operation), and hclust is then used to create a hierarchical clustering from that distance matrix. The result is stored in MD.vclust.

barchart(MD.m4, shade = TRUE,which = rev(MD.vclust$order)) #generates a barchart using the object MD.k4, which likely contains the clustering output from a previous k-means analysis. 
#shade = TRUE argument adds shading to the bars to give a visual indication of significance or magnitude. 
#which = rev(MD.vclust$order) argument reorders the barchart to match the order of clusters determined by the hierarchical clustering object MD.vclust. This reordering is done in reverse (rev) to perhaps match a specific ordering convention or for better visual clarity.
```
o	Describing segments
o	Selecting (the) target segment(s)
o	Customising the marketing mix
o	Evaluation and monitoring



#V Segmentation
## McD Example

```{r}
# R script for the analysis of the McDonald's example in LCA II lecture
library(flexmix)
library("flexclust")
rm(list=ls())
mcdonalds <- read_csv("Datasets/mcdonalds.csv")
```


EXACT SAME AS III SEGMENTATION
```{r data insights}
# Look into content
names(mcdonalds)
dim(mcdonalds)
head(mcdonalds, 3)
```

EXACT SAME AS III SEGMENTATION
```{r Discretize Yes:1 NO:0 }
# Change Yes/No to 1/0
MD.x <- as.matrix(mcdonalds[, 1:11]) #function converts this selected subset of the data frame into a matrix called MD.x. mcdonalds[, 1:11] selects all rows and the first 11 columns of the mcdonalds data frame.
MD.x <- (MD.x == "Yes") + 0 # TRUE can be coerced to 1 and FALSE to 0. Therefore, adding 0 (+ 0) to the logical matrix effectively converts all TRUE values to 1s and all FALSE values to 0s.
head(MD.x, 3)
```

EXACT SAME AS III SEGMENTATION
```{r Column means }
## Exploring data
# What fraction sees McDonald's possessing each of the attributes
round(colMeans(MD.x), 2) #rounding the column means to two decimals
```
Column Means:
yummy convenient      spicy  fattening     greasy 
0.55       0.91       0.09       0.87       0.53
      
fast      cheap      tasty  expensive    healthy  disgusting
0.90       0.60       0.64       0.36       0.20    0.24

EXACT SAME AS III SEGMENTATION
```{r PCA}
# PCA and perceptual map
MD.pca <- prcomp(MD.x)
summary(MD.pca)
```
PCA Principal Component analysis:
- The idea behind PCA is to reduce the dimensionality of the dataset by transforming the original variables into a new set of variables (the principal components) which are uncorrelated, and which are ordered so that the first few retain most of the variation present in all of the original variables.
- The greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.
Standard Deviation:
- This indicates how much variance is captured by each principal component. A higher standard deviation means that component accounts for a larger variance within the dataset.
Proportion of Variance:
- squared standard deviation for each component divided by the sum of squared standard deviations of all components. It tells us the proportion of the dataset’s total variance that is captured by each principal component.

EXACT SAME AS III SEGMENTATION
```{r Rotation/Loadings PCA}
print(MD.pca, digits = 2)
```
Rotation matrix:
- Each row corresponds to one of the original variables (attributes like "yummy", "convenient", "spicy", etc.), and each column corresponds to a principal component (PC1 to PC11). The values in the matrix show how each attribute contributes to each principal component.
Example (original variable yummy on PC1):
- The attribute "yummy" has a strong positive loading on PC1 (0.4769) and a strong negative loading on PC9 (-0.5724), which means "yummy" is an important attribute that defines the direction of PC1 and PC9.

EXACT SAME AS III SEGMENTATION
```{r PCA loading-plot}
plot(predict(MD.pca), col = "grey")
projAxes(MD.pca)
```
- dots represent the data points projected onto the first two principal components, PC1 and PC2. This provides a two-dimensional summary of the data, where similar data points are clustered together, and different ones are farther apart.
- arrows (or vectors) represent the loadings of each attribute (like "yummy", "cheap", "expensive", etc.), which point in the direction of the maximum variance for that attribute. The length of an arrow indicates the strength of the attribute's contribution to the two principal components displayed.
- Attributes that are close together are positively correlated, while those that are on opposite ends of the plot are negatively correlated.
- Attributes that are close to the origin (center of the plot) have smaller effects on the variation explained by the first two principal components, while those further out are more influential.

It is seen that "tasty" and "yummy" point in a similar direction, suggesting they contribute similarly to the dataset's variance and are likely correlated. Conversely, "expensive" points in a roughly opposite direction to "cheap", indicating they are negatively correlated. The plot also suggests that attributes like "disgusting" and "fattening" may not be strongly related to the primary variance within the data, as their vectors are shorter and closer to the center of the plot.


Extracting segments
##Mixture REGRESSION
This lecture will help you to understand
-	Other finite mixture models
-	Applications of mixture models

   Morten on Finite Mixture models:
o	The “classical” segmentation approach (hierarchical and nonhierarchical) resembles in many ways the latent class analysis/finite mixtures of distributions and result often in comparable solutions
o	The same kind of input data can be used and the overall segmentation process is very similar
o	A related set of methods, finite mixtures of regression models, takes a rather different approach
As the name suggests, we have a dependent variable and a set of independent variables, but the functional relationship between them depends on which of a finite set of segments the object belongs to

Morten McDonalds Case:
o	Instead of finding market segments with similar perceptions we could consider finding segments with members whose love/hate for McDonald’s is driven by similar perceptions
o	In this situation, McDonald’s could try to modify critical perceptions for certain segments with the purpose of improving love
o	This idea can be operationalized using finite mixtures of linear regression models
o	The dependent variable is the degree to which consumers love McDonald’s and the independent variables are the 11 perceptions
The segmentation variables are unobserved and consist of the regression coefficients

like Frequency table
 - displays a frequency table of these levels.
```{r like Frequency table}
#Need to fix the representation of like and use "like" (not "Like") onwards
mcdonalds$like <- factor(
  mcdonalds$Like,levels=c("I love it!+5","+4","+3","+2","+1","0","-1","-2","-3","-4","I hate it!-5")) #modifies the factor levels of a variable (like) in the mcdonalds dataset and then
rev(table(mcdonalds$like))
```
"I love it!+5": 143 observations.
"+4":           160 observations.
"+3":           229 observations.
"+2":           187 observations.
"+1":           152 observations.
"0":            169 observations.
"-1":           152 observations.
"-2":           59 observations.
"-3":           73 observations.
"-4":           71 observations.
"I hate it!-5": 152 observations.

Morten on McDonalds - Numeric transformation
o	The ordinal nature of the dependent variable causes problems – the larger number of segments we investigate the more likely we end up in a situation where a segment is made up of consumers with identical rating on the dependent variable
o	Such a group can be perfectly predicted and would lead to an infinite log-likelihood – a degenerate solution
For this reason we will have to settle with a 2 segment solution

Numeric like frquency table
 - transforms the like variable from a factor to a numeric variable and then adjusts the numeric values. 
 - displays a frequency table of the transformed like variable.
```{r Numeric like frquency table }
mcdonalds$like.n <- 6 - as.numeric(mcdonalds$like)
table(mcdonalds$like.n)
```
This is done in preparation of the flowwing Regression

Prepares a regression formula for use in finite mixture of linear regression model
```{r}
f <- paste(names(mcdonalds)[1:11], collapse = "+")
#names(mcdonalds)[1:11] Retrieves the names of the first 11 columns in the mcdonalds dataset. These columns are presumably predictors for the regression model.
#collapse = "+"): Combines these column names into a single string, with each name separated by a + sign. This string represents the right-hand side of the regression formula.
f <- paste("like.n ~ ", f, collapse = "")
# collapse = "" ensures there are no extra spaces added.
f <- as.formula(f)
f
```
Dependent Variable (like.n):
 - transformed like variable, now numeric, which represents the level of liking for McDonald's.
Independent Variables: 
 - The predictors in the model are the first 11 columns from the mcdonalds dataset:
 
Finite mixture linear regression model
 - performs a finite mixture of linear regression models using the stepFlexmix function
```{r Fit Finite mixture linear regression model}
set.seed(1234)
MD.reg2 <- stepFlexmix(f, 
                       data = mcdonalds, 
                       k = 2, #number of clusters
                       nrep = 10, 
                       verbose = FALSE)
MD.reg2
```
Cluster 1: 630 observations.
Cluster 2: 823 observations.
SUMMARY:
 - clusters represent groups of observations that have similar relationships between the predictor variables and the response variable (like.n).

Refitting the finite mixture of linear regression models and summarizing the results for each component
```{r Refit finite mixture of linear regression model }
MD.ref2 <- refit(MD.reg2)
summary(MD.ref2)
```
Output 
 - provides the regression coefficients for each component of the mixture model. The output is divided into two components, 
Comp.1
 - yummyYes, tastyYes, cheapYes, and disgustingYes are significant predictors for like.n with high significance levels.
 - Positive coefficients (e.g., yummyYes, tastyYes) suggest that these attributes positively influence the like.n score.
 - Negative coefficients (e.g., fatteningYes, greasyYes, disgustingYes) indicate a negative influence on the like.n score.

Comp.2
 - yummyYes, convenientYes, fatteningYes, greasyYes, healthyYes, and disgustingYes are significant predictors for like.n.
 - Similar to Comp.1, positive coefficients (e.g., yummyYes, convenientYes, healthyYes) indicate a positive influence on like.n.
 - Negative coefficients (e.g., fatteningYes, greasyYes, disgustingYes) indicate a negative influence on like.n.
SUMMARY:
 - the refitted model MD.ref2 shows the estimated coefficients for each predictor variable in both components of the mixture model. 
 - These coefficients indicate the direction and magnitude of the influence of each predictor on the like.n score. 
 - Significant predictors can be identified by their low p-values, marked with ***, **, or *, indicating strong evidence against the null hypothesis that the coefficient is zero. 
 - This detailed information helps in understanding the different segments within the data and their distinct relationships between the predictors and the response variable (like.n)


Plot Refit finite mixture of linear regression model
 - visualizes the coefficients for each predictor variable in the two components of the mixture model, indicating their significance.
```{r Plot Refit finite mixture of linear regression model  }
plot(MD.ref2, significance = TRUE)
```
 - x-axis represents the coefficient values, ranging from -4 to 6.

Bars:
 - each bar represents the estimated coefficient for a predictor variable.
 - length and direction (positive or negative) of the bar indicate the magnitude and direction of the effect of the predictor on the response variable (like.n).
 - darker bars indicate statistically significant coefficients, while lighter bars represent non-significant coefficients.
SUMMARY
 - plot provides a clear visual representation of the coefficients for each predictor variable in the two components of the mixture model. 
 - highlights which predictors have a significant effect on the response variable (like.n) in each component. 
 - visualization helps in understanding the different influences of the predictors across the identified segments and can guide targeted strategies based on these insights

Morten SUMMARY:
o	Group 1 like McDonald’s if they perceive it as YUMMY, not FATTENING, FAST, CHEAP, TASTY, and not DISGUSTING
o	Group 2 like McDonald’s if they perceive it as YUMMY, CONVENIENT, not GREASY, HEALTHY, and not DISGUSTING
o	So if segment 2 is targeted it is important to stress that McDonald’s serves some healthy food – this is not necessary in order to target segment 1
To target segment 1, McDonald’s should stress how tasty, fast and cheap the food is


THE REMAINING STEPS
-	See previous slide set for considerations regarding
o	Profiling segments
o	Describing segments
o	Selecting (the) target segment(s)
o	Customising the marketing mix
o	Evaluation and monitoring


