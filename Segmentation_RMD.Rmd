---
title: "Segmentation_RMD"
output: html_document
date: "2024-04-11"
---

# HBAT dataset
HBAT is a manufacturer of paper products who sells products to two market segments: the newsprint industry and the magazine industry. The data used in this example is based on a survey of HBAT customers who completed a questionnaire on a website. 100 customers (purchasing managers from firms) buying from HBAT completed the questionnaire.
-	The first type of information is available from HBAT’s data warehouse and includes information, such as, size of the customer and length of purchase relationship
-	The second type of information was consumers perceptions of HBAT’s performance on 13 attributes using a 0-10 scale with 10 being “Excellent” and 0 being “Poor”
-	The third type of information relates to purchase outcomes and business relationships (e.g. satisfaction with HBAT, and whether the firm would consider a strategic alliance/partnership with HBAT)

```{r dataload}
# R script for the analysis of the HBAT example in the cluster analysis lecture
#install.packages("readstata13")
#install.packages("NbClust")

library(readstata13)
library(NbClust)
rm(list=ls())

HBAT <- read.dta13("Datasets/hbat.dta")
#HBAT <- read.dta13("C:/Users/Bruger/OneDrive - Aarhus universitet/8. semester - BI/CA - Customer Analytics/Segmentation/hbat.dta") #This also works
```

```{r Variable selection}
# first we make our own id variable based on the original order
nobs <- nrow(HBAT) #calculates the number of observations (rows) in the dataset HBAT and assigns it to the variable nobs.
HBAT$id <- seq(1,nobs) #a new column id is added to the HBAT data frame. It is filled with a sequence of numbers from 1 to nobs, effectively creating a unique identifier for each row based on its original order in the data frame

# pick the variables for cluster analysis and summarize them
hbat <- c("x6","x8","x12","x15","x18") # a vector
summary(HBAT[,hbat])
```
X6: Product Quality
X8: Technical Support 
X12: Salesforce Image 
X15: New Products
X18: Delivery Speed
In "dataload" the summary stats for selection variables are shown.

```{r SD and Multicollinearity}
apply(HBAT[,hbat],2,sd)

# multicollinearity should not be a problem
cor(HBAT[,hbat])
# which is confirmed
```
What is the feasible value for ST in this example???
- 
What is the threshold value before having problems with multicollinearity?
- From chat: Values above 0.7 or 0.8 (or below -0.7 or -0.8) are often considered high, indicating potential multicollinearity problems.
Values between 0.5 and 0.7 (or -0.5 and -0.7) suggest moderate correlation, but they might still be acceptable depending on the analysis.

```{r Outlier detection}
# look for outliers
dev <- t(t(HBAT[,hbat])-apply(HBAT[,hbat],2,mean))
dev2 <- dev^2
sumdev2 <- rowSums(dev2)
sort(sqrt(sumdev2))
# observations 6 and 87 are candidates for deletion
```
The outliers are assessed in the relationship of the outliers being next to or before in the sequence.
the distance between obs 90 and 6 is 0.7, which is a value giving the analyst the possibility to extract obs 6 from the dataset.

## Ward's method - Hierarchical clustering
Ward’s method calculates the within-cluster sums of squares for all possible five-cluster solutions:
(I think we get 5 possible cluster solutions having 6 variables, remember for different exercises)
```{r Ward's method - Dendrogram}
HBAT <- subset(HBAT,id!="6"&id!="87") #extracting variable 6 and 87
nobs <- nrow(HBAT) 
HBAT$id <- seq(1,nobs) #number of observations (nobs) is recalculated, and the id column is reinitialized to have a sequential order starting from 1 up to the number of observations.

## Hierarchical clustering
# first create distance matrix
dist <- dist(HBAT[,hbat],method="euclidean") #dist function calculates the Euclidean distance between observations using the selected variables (hbat). This creates a distance matrix that hierarchical clustering algorithms use to determine how observations group together.
dist2 <- dist^2 #Ward's method is sensitive to the scale of the distances, and squaring the distances emphasizes larger distances.

# Wards method
H.fit <- hclust(dist2,method="ward.D") #Ward's method minimizes the total within-cluster variance.

# draw a dendogram
plot(H.fit) #Assess the number of clusters from this plot
```
I need an assessment-method on determining the number of cluster detected by the plot??

```{r Wards - pct increase}
# assess pct. increase
denominator <- cumsum(H.fit[[2]]) #calculates the cumulative sum of the heights (or distances) at which each clustering step takes place. In Ward's method, these heights represent the increase in the total within-cluster variance when two clusters are merged. H.fit[[2]] contains the heights of the merges at each step.

length(denominator) <- nobs-2 #length of the denominator vector is set to be one less than the number of observations in the data because the hierarchical clustering process involves n-1 merge steps.

denominator <- c(1,denominator) #A '1' is prepended to the denominator vector. This ensures that when calculating percentage increases, the first step (where no previous clustering has occurred) is properly handled by having a base of '1' to compare against.

pct <- H.fit[[2]]/denominator #calculates the percentage increase in the height at each step of the clustering process by dividing the height of each merge by the cumulative sum up to the previous step. This percentage increase can be interpreted as how much 'worse' the clustering gets at each step, as measured by Ward's distance metric.

tail(pct,n=10) #displays the last 10 values of the pct vector. These values are the percentage increases for the final 10 steps in the hierarchical clustering process.
```
NOT understand how we assess that 4 clusters is optimal from this percentage vector???

```{r wards - create cluster groups}
# apart from going from 2-1 the largest jump is from 4-3, we stop just before
grp <- as.factor(cutree(H.fit,k=4))
table(grp) #majority is in first cluster having 49 obs
```

```{r Wards - Dendrogram red borders}
# illustrate a 4 cluster solution
plot(H.fit)
rect.hclust(H.fit,k=4,border="red") #smart plot
```


```{r Wards - Assess cluster groups}
# assess outcome
aggregate(HBAT[,hbat],list(grp),mean)
```
Here it is seen that x6 Product quality is
- scoring highest in Cluster 2
- scoring lowest in Cluster 3 (etc.)
Here it is seen that x8 Technical Support is
- scoring highest in Cluster 2
- scoring lowest in Cluster 4 (etc.)

Group 1 has observations yielding high impact from x6 Product Quality and low impact from x8 Technical Support
Group 2 has observations yielding high impact from x6 Product Quality and high impact from x8 Technical Support (etc.)

```{r Wards - ANOVA on variables}
summary(aov(x6~grp,data=HBAT))
summary(aov(x8~grp,data=HBAT))
summary(aov(x12~grp,data=HBAT))
summary(aov(x15~grp,data=HBAT))
summary(aov(x18~grp,data=HBAT))
```
The ANOVA shows, that all variables except x18 Delivery speed have significant difference within the mean of the groups.
eg. x6 has means which differs in the group, as this group may be more volatile due to the scores given in each observation in the group
eg. x18 has means which is quite close to each other, yeilding a score where the observations are quite similar

## Complete-linkage method - Hierarchical clustering
-	Complete-linkage/farthest-neighbor measures the distance between clusters as the maximum of the distance between all possible pairs of objects in the two clusters – tends to find compact clusters with equal diameters
-	These methods only depend on the ordinal properties of the distances
```{r Complete-linkage method - Dendrogram }
# Complete linkage
H.fit <- hclust(dist,method="complete")
# draw a dendogram
plot(H.fit)
```


```{r Complete - pct increase }
# assess pct. increase
denominator <- cumsum(H.fit[[2]]) #calculates the cumulative sum of the heights (or distances) at which each clustering step takes place. In Ward's method, these heights represent the increase in the total within-cluster variance when two clusters are merged. H.fit[[2]] contains the heights of the merges at each step.

length(denominator) <- nobs-2 #length of the denominator vector is set to be one less than the number of observations in the data because the hierarchical clustering process involves n-1 merge steps.

denominator <- c(1,denominator) #A '1' is prepended to the denominator vector. This ensures that when calculating percentage increases, the first step (where no previous clustering has occurred) is properly handled by having a base of '1' to compare against.

pct <- H.fit[[2]]/denominator #calculates the percentage increase in the height at each step of the clustering process by dividing the height of each merge by the cumulative sum up to the previous step. This percentage increase can be interpreted as how much 'worse' the clustering gets at each step, as measured by Ward's distance metric.

tail(pct,n=10) ##displays the last 10 values of the pct vector. These values are the percentage increases for the final 10 steps in the hierarchical clustering process.
```
NOT understand how we assess that 4 clusters is optimal from this percentage vector???


```{r Complete - create cluster groups }
# results from Ward's method are supported -
grp <- as.factor(cutree(H.fit,k=4))
table(grp) #majority is in second cluster having 33 obs
```

```{r Complete - Dendrogram red borders }
# but size of the clusters differ
plot(H.fit)
rect.hclust(H.fit,k=4,border="red")
```


```{r Complete - Assess cluster groups }
# assess outcome
aggregate(HBAT[,hbat],list(grp),mean)
```
Here it is seen that x6 Product quality is
- scoring highest in Cluster 2
- scoring lowest in Cluster 4 (etc.)
Here it is seen that x8 Technical Support is
- scoring highest in Cluster 2
- scoring lowest in Cluster 1 (etc.)

Group 1 has observations yielding high impact from x6 Product Quality and low impact from x8 Technical Support
Group 2 has observations yielding high impact from x6 Product Quality and medium impact from x8 Technical Support (etc.)

```{r Complete - Dendrogram red borders }
summary(aov(x6~grp,data=HBAT))
summary(aov(x8~grp,data=HBAT))
summary(aov(x12~grp,data=HBAT))
summary(aov(x15~grp,data=HBAT))
summary(aov(x18~grp,data=HBAT))
```
The ANOVA shows, that all variables except x18 Delivery speed have significant difference within the mean of the groups.
eg. x6 has means which differs in the group, as this group may be more volatile due to the scores given in each observation in the group
eg. x18 has means which is quite close to each other, yeilding a score where the observations are quite similar

- IT IS RECOOMENDED THAT A COMBINATION OF THE HIERARCHICAL AND NONHIERARCHICAL METHODS IS USED
o	Use the hierarchical methods to get a qualified estimate on the number of clusters
o	Use the nonhierarchical methods to determine the cluster affiliation (tilknytning) for each object

BOOKMARK 15/4-24
--NOT FINISH--
```{r Complete - Dendrogram red borders }
# use NbClust
res<-NbClust(HBAT[,hbat], distance = "euclidean", min.nc=2, max.nc=8, 
             method = "ward.D", index = "all") #go with the printed number of clusters
res$All.index
res$Best.nc
```

#Non-hierarchical clusters

```{r Complete - Dendrogram red borders }
## Nonhierarchical clustering
set.seed(4118)
NH.fit<-kmeans(HBAT[,hbat],4,nstart=25)
print(NH.fit)
grp <- as.factor(NH.fit[[1]])
table(grp)
# assess outcome
aggregate(HBAT[,hbat],list(grp),mean)
summary(aov(x6~grp,data=HBAT))
summary(aov(x8~grp,data=HBAT))
summary(aov(x12~grp,data=HBAT))
summary(aov(x15~grp,data=HBAT))
summary(aov(x18~grp,data=HBAT))
# snake plot
matplot(t(NH.fit[[2]]),type="l")
# criterion validity
aggregate(HBAT[,c("x19","x20","x21","x22")],list(grp),mean)
summary(aov(x19~grp,data=HBAT))
summary(aov(x20~grp,data=HBAT))
summary(aov(x21~grp,data=HBAT))
summary(aov(x22~grp,data=HBAT))
# profiling
tbl <- table(HBAT$x1,grp)
round(100*prop.table(tbl,2))
chisq.test(tbl)
tbl <- table(HBAT$x2,grp)
round(100*prop.table(tbl,2))
chisq.test(tbl)
tbl <- table(HBAT$x3,grp)
round(100*prop.table(tbl,2))
chisq.test(tbl)
tbl <- table(HBAT$x4,grp)
round(100*prop.table(tbl,2))
chisq.test(tbl)
tbl <- table(HBAT$x5,grp)
round(100*prop.table(tbl,2))
chisq.test(tbl)


## Toy example
inc <- c(5,6,15,16,25,30)
edu <- c(5,6,14,15,20,19)
toy <- data.frame(inc,edu)
toy.dist <- dist(toy,method="euclidean")
toy.dist2 <- toy.dist^2
toy.H.single <- hclust(toy.dist2,method="single")
# the actual amalgamation
toy.H.single$merge
# the associated increase in distance/heterogeneity
toy.H.single$height
# a permutation of the original observations suitable for plotting
toy.H.single$order
# plot
plot(toy.H.single)

# other linkage possibilities
toy.H.complete <- hclust(toy.dist,method="complete")
toy.H.average <- hclust(toy.dist,method="average")
toy.H.centroid <- hclust(toy.dist,method="centroid")
# Wards method
toy.H.ward <- hclust(toy.dist2,method="ward.D")

## Distance vs shape
scores<-matrix(c(21,34,17,42,62,75,58,85),nrow=4,byrow=F)
matplot(scores,type="l")
dist(t(scores),method="euclidean")
cor(scores)

```

#II Segmentation
## McD Example

```{r 20.segmentation}
# R script for the analysis of the HBAT example in the model-based clustering
# lecture
library(readstata13)
rm(list=ls())

## Preparation
HBAT <- read.dta13("c:/users/au78328/OneDrive - Aarhus Universitet/home/customer_analytics/s24/analysis/hbat.dta")
# first we make our own id variable based on the original order
nobs <- nrow(HBAT)
HBAT$id <- seq(1,nobs)
# pick the variables for cluster analysis and summarize them
hbat <- c("x6","x8","x12","x15","x18")
summary(HBAT[,hbat])
apply(HBAT[,hbat],2,sd)
# multicollinearity should not be a problem
cor(HBAT[,hbat])
# which is confirmed
# look for outliers
dev <- t(t(HBAT[,hbat])-apply(HBAT[,hbat],2,mean))
dev2 <- dev^2
sumdev2 <- rowSums(dev2)
sort(sqrt(sumdev2))
# observations 6 and 87 are candidates for deletion
HBAT <- subset(HBAT,id!="6"&id!="87")
nobs <- nrow(HBAT)
HBAT$id <- seq(1,nobs)

## Model-based clustering
library("mclust")
(mc <- Mclust(HBAT[,hbat]))
# Assess different solutions
plot(mc, HBAT[,hbat], what = "BIC", col = "black")

# Summarize
summary(mc)

# Look at a 4-cluster solution
mc4 <- Mclust(HBAT[,hbat],G=4)
summary(mc4)

```
--NOT FINISH--

#III Segmentation
## McD Example

```{r dataload McD}
# R script for the analysis of the McDonald's example in the segmentation lecture
#install.packages("flexclust")
#install.packages("partykit")
#install.packages("clue") #- should be installed as part of the flexclust but isn't
library("flexclust")
library("partykit")
library("clue")
rm(list=ls())

## Preparation - Read in data
library(readr)
mcdonalds <- read_csv("Datasets/mcdonalds.csv")
```


```{r data insights}
# Look into content
names(mcdonalds)
dim(mcdonalds)
head(mcdonalds, 3) #showing the first 3 rows in dataset mcdonalds
```


```{r Discretize Yes:1 + Column means }
# Change Yes/No to 1/0
MD.x <- as.matrix(mcdonalds[, 1:11]) #function converts this selected subset of the data frame into a matrix called MD.x. mcdonalds[, 1:11] selects all rows and the first 11 columns of the mcdonalds data frame.
MD.x <- (MD.x == "Yes") + 0 # TRUE can be coerced to 1 and FALSE to 0. Therefore, adding 0 (+ 0) to the logical matrix effectively converts all TRUE values to 1s and all FALSE values to 0s.

## Exploring data
# What fraction sees McDonald's possessing each of the attributes
round(colMeans(MD.x), 2) #rounding the column means to two decimals
```


```{r PCA}
# PCA and perceptual map
MD.pca <- prcomp(MD.x)
summary(MD.pca)
```
PCA Principal Component analysis:
- The idea behind PCA is to reduce the dimensionality of the dataset by transforming the original variables into a new set of variables (the principal components) which are uncorrelated, and which are ordered so that the first few retain most of the variation present in all of the original variables.
- The greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.
Standard Deviation:
- This indicates how much variance is captured by each principal component. A higher standard deviation means that component accounts for a larger variance within the dataset.
Proportion of Variance:
- squared standard deviation for each component divided by the sum of squared standard deviations of all components. It tells us the proportion of the dataset’s total variance that is captured by each principal component.

```{r Rotation/Loadings PCA}
print(MD.pca, digits = 2)
```
Rotation matrix:
- Each row corresponds to one of the original variables (attributes like "yummy", "convenient", "spicy", etc.), and each column corresponds to a principal component (PC1 to PC11). The values in the matrix show how each attribute contributes to each principal component.
Example (original variable yummy on PC1):
- The attribute "yummy" has a strong positive loading on PC1 (0.4769) and a strong negative loading on PC9 (-0.5724), which means "yummy" is an important attribute that defines the direction of PC1 and PC9.

```{r 21 segmentation}
plot(predict(MD.pca), col = "grey")
projAxes(MD.pca)
```
- dots represent the data points projected onto the first two principal components, PC1 and PC2. This provides a two-dimensional summary of the data, where similar data points are clustered together, and different ones are farther apart.
- arrows (or vectors) represent the loadings of each attribute (like "yummy", "cheap", "expensive", etc.), which point in the direction of the maximum variance for that attribute. The length of an arrow indicates the strength of the attribute's contribution to the two principal components displayed.
- Attributes that are close together are positively correlated, while those that are on opposite ends of the plot are negatively correlated.
- Attributes that are close to the origin (center of the plot) have smaller effects on the variation explained by the first two principal components, while those further out are more influential.

It is seen that "tasty" and "yummy" point in a similar direction, suggesting they contribute similarly to the dataset's variance and are likely correlated. Conversely, "expensive" points in a roughly opposite direction to "cheap", indicating they are negatively correlated. The plot also suggests that attributes like "disgusting" and "fattening" may not be strongly related to the primary variance within the data, as their vectors are shorter and closer to the center of the plot.

BOOKMARK 18/4-24

```{r 21 segmentation}
## Extracting segments
# k-means
set.seed(28)
MD.km28 <- stepFlexclust(MD.x, 2:8, nrep = 10,verbose = FALSE)
plot(MD.km28, xlab = "number of segments")
```


```{r 21 segmentation}
# Segment stability within the same number of segments
set.seed(1234)
MD.b28 <- bootFlexclust(MD.x, 2:8, nrep = 10,nboot = 100)
plot(MD.b28, xlab = "number of segments",ylab = "adjusted Rand index")
```


```{r 21 segmentation}
# Gorge plot
MD.km28 <- relabel(MD.km28)
histogram(MD.km28[["4"]], data = MD.x, xlim = 0:1)
#the plots should look like a gorge
```


```{r 21 segmentation}
# Segment level stability across solutions
slsaplot(MD.km28)
```


```{r 21 segmentation}
# Save the four-segment solution
MD.k4 <- MD.km28[["4"]]
MD.r4 <- slswFlexclust(MD.x, MD.k4)
plot(MD.r4, ylim = 0:1, xlab = "segment number",ylab = "segment stability")
```


```{r 21 segmentation}
## Profiling segments
# Segment profile plot
MD.vclust <- hclust(dist(t(MD.x)))
barchart(MD.k4, shade = TRUE,which = rev(MD.vclust$order))
```


```{r 21 segmentation}
# Segment separation plot
plot(MD.k4, project = MD.pca, data = MD.x,hull = FALSE, simlines = FALSE,
     xlab = "principal component 1",ylab = "principal component 2")
projAxes(MD.pca)
```


```{r 21 segmentation}
## Describing segments
# Extract the segment membership for each consumer
mcdonalds$k4 <- clusters(MD.k4)
# Prepare the like/hate variable
mcdonalds$like <- factor(mcdonalds$Like,levels=c("I love it!+5","+4","+3","+2","+1","0","-1","-2","-3","-4","I hate it!-5"))
# Make mosaic plot of like/hate
mosaicplot(table(mcdonalds$k4, mcdonalds$like), shade = TRUE,main = "", 
           xlab = "segment number")
```


```{r 21 segmentation}
# Make mosaic plot of gender
mosaicplot(table(mcdonalds$k4, mcdonalds$Gender), shade = TRUE)
boxplot(mcdonalds$Age ~ mcdonalds$k4,varwidth = TRUE, notch = TRUE)
```


```{r 21 segmentation}
## Predict segment 3 membership using a classification tree
# Prepare visit frequency variable
mcdonalds$visitfrequency <- factor(mcdonalds$VisitFrequency,
                                   levels=c("Never","Once a year","Every three months","Once a month","Once a week","More than once a week"))
tree <- ctree(factor(k4==3) ~ like+Age+visitfrequency+factor(Gender),
              data=mcdonalds)
plot(tree)
```


```{r 21 segmentation}
## Selecting (the) target segment(s)
# Visit frequency is scored as 1=Never,...,6=More than once a week, 
# not strictly aligned with the ratio properties hidden in the scale
visit <- tapply(as.numeric(mcdonalds$visitfrequency),mcdonalds$k4,mean)
visit
```


```{r 21 segmentation}
# Turn the current like scale upside down and recenter 
# (1=I love it,...,11=I hate it -> 5=I love it,...,-5=I hate it)
like <- tapply(-as.numeric(mcdonalds$like)+6,mcdonalds$k4,mean)
like
```


```{r 21 segmentation}
# Create the gender variable
female <- tapply((mcdonalds$Gender=="Female")+0,mcdonalds$k4,mean)
female
```


```{r 21 segmentation}
# Make segment evaluation plot
plot(visit, like, cex = 10 * female,xlim = c(2, 4.5), ylim = c(-3, 3))
text(visit, like, 1:4)

```

#IV Segmentation
## McD Example

```{r}
# R script for the analysis of the McDonald's example in LCA I lecture
library("flexclust")
library("flexmix")
rm(list=ls())

## Preparation
# Read in data
mcdonalds <- read.csv("c:/users/au78328/OneDrive - Aarhus Universitet/home/customer_analytics/s24/analysis/mcdonalds.csv")
# Look into content
names(mcdonalds)
dim(mcdonalds)
head(mcdonalds, 3)
# Change Yes/No to 1/0
MD.x <- as.matrix(mcdonalds[, 1:11])
MD.x <- (MD.x == "Yes") + 0

## Exploring data
# What fraction sees McDonald's possessing each of the attributes
round(colMeans(MD.x), 2)
# PCA and perceptual map
MD.pca <- prcomp(MD.x)
summary(MD.pca)
print(MD.pca, digits = 1)
plot(predict(MD.pca), col = "grey")
projAxes(MD.pca)

## Extracting segments
# k-Means
set.seed(28)
MD.km28 <- stepFlexclust(MD.x, 2:8, nrep = 10,verbose = FALSE)
plot(MD.km28, xlab = "number of segments")
# Segment stability within the same number of segments
set.seed(1234)
MD.b28 <- bootFlexclust(MD.x, 2:8, nrep = 10,nboot = 100)
plot(MD.b28, xlab = "number of segments",ylab = "adjusted Rand index")
# Gorge plot
MD.km28 <- relabel(MD.km28)
histogram(MD.km28[["4"]], data = MD.x, xlim = 0:1)
# Segment level stability across solutions
slsaplot(MD.km28)
# Save the four-segment solution
MD.k4 <- MD.km28[["4"]]
MD.r4 <- slswFlexclust(MD.x, MD.k4)
plot(MD.r4, ylim = 0:1, xlab = "segment number",ylab = "segment stability")
# mixture distribution
set.seed(1234)
MD.m28 <- stepFlexmix(MD.x ~ 1, k = 2:8, nrep = 10, model = FLXMCmvbinary(), 
                      verbose = FALSE)
MD.m28
plot(MD.m28,ylab = "value of information criteria (AIC, BIC, ICL)")
MD.m4 <- getModel(MD.m28, which = "4")
table(kmeans = clusters(MD.k4),mixture = clusters(MD.m4))
MD.m4a <- flexmix(MD.x ~1, cluster = clusters(MD.k4),model = FLXMCmvbinary())
table(kmeans = clusters(MD.k4),mixture = clusters(MD.m4a))
logLik(MD.m4a)
logLik(MD.m4)
```






