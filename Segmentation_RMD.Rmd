---
title: "Segmentation_RMD"
output: html_document
date: "2024-04-11"
---

# HBAT dataset
HBAT is a manufacturer of paper products who sells products to two market segments: the newsprint industry and the magazine industry. The data used in this example is based on a survey of HBAT customers who completed a questionnaire on a website. 100 customers (purchasing managers from firms) buying from HBAT completed the questionnaire.
-	The first type of information is available from HBAT’s data warehouse and includes information, such as, size of the customer and length of purchase relationship
-	The second type of information was consumers perceptions of HBAT’s performance on 13 attributes using a 0-10 scale with 10 being “Excellent” and 0 being “Poor”
-	The third type of information relates to purchase outcomes and business relationships (e.g. satisfaction with HBAT, and whether the firm would consider a strategic alliance/partnership with HBAT)

```{r dataload}
# R script for the analysis of the HBAT example in the cluster analysis lecture
#install.packages("readstata13")
#install.packages("NbClust")

library(readstata13)
library(NbClust)
rm(list=ls())

HBAT <- read.dta13("Datasets/hbat.dta")
#HBAT <- read.dta13("C:/Users/Bruger/OneDrive - Aarhus universitet/8. semester - BI/CA - Customer Analytics/Segmentation/hbat.dta") #This also works
```

```{r Variable selection}
# first we make our own id variable based on the original order
nobs <- nrow(HBAT) #calculates the number of observations (rows) in the dataset HBAT and assigns it to the variable nobs.
HBAT$id <- seq(1,nobs) #a new column id is added to the HBAT data frame. It is filled with a sequence of numbers from 1 to nobs, effectively creating a unique identifier for each row based on its original order in the data frame

# pick the variables for cluster analysis and summarize them
hbat <- c("x6","x8","x12","x15","x18") # a vector
summary(HBAT[,hbat])
```
X6: Product Quality
X8: Technical Support 
X12: Salesforce Image 
X15: New Products
X18: Delivery Speed
In "dataload" the summary stats for selection variables are shown.

```{r SD and Multicollinearity}
apply(HBAT[,hbat],2,sd)

# multicollinearity should not be a problem
cor(HBAT[,hbat])
# which is confirmed
```
What is the feasible value for ST in this example???
- 
What is the threshold value before having problems with multicollinearity?
- From chat: Values above 0.7 or 0.8 (or below -0.7 or -0.8) are often considered high, indicating potential multicollinearity problems.
Values between 0.5 and 0.7 (or -0.5 and -0.7) suggest moderate correlation, but they might still be acceptable depending on the analysis.

```{r Outlier detection}
# look for outliers
dev <- t(t(HBAT[,hbat])-apply(HBAT[,hbat],2,mean))
dev2 <- dev^2
sumdev2 <- rowSums(dev2)
sort(sqrt(sumdev2))
# observations 6 and 87 are candidates for deletion
```
The outliers are assessed in the relationship of the outliers being next to or before in the sequence.
the distance between obs 90 and 6 is 0.7, which is a value giving the analyst the possibility to extract obs 6 from the dataset.

## Ward's method - Hierarchical clustering
Ward’s method calculates the within-cluster sums of squares for all possible five-cluster solutions:
(I think we get 5 possible cluster solutions having 6 variables, remember for different exercises)
```{r Ward's method - Dendrogram}
HBAT <- subset(HBAT,id!="6"&id!="87") #extracting variable 6 and 87
nobs <- nrow(HBAT) 
HBAT$id <- seq(1,nobs) #number of observations (nobs) is recalculated, and the id column is reinitialized to have a sequential order starting from 1 up to the number of observations.

## Hierarchical clustering
# first create distance matrix
dist <- dist(HBAT[,hbat],method="euclidean") #dist function calculates the Euclidean distance between observations using the selected variables (hbat). This creates a distance matrix that hierarchical clustering algorithms use to determine how observations group together.
dist2 <- dist^2 #Ward's method is sensitive to the scale of the distances, and squaring the distances emphasizes larger distances.

# Wards method
H.fit <- hclust(dist2,method="ward.D") #Ward's method minimizes the total within-cluster variance.

# draw a dendogram
plot(H.fit) #Assess the number of clusters from this plot
```
I need an assessment-method on determining the number of cluster detected by the plot??

```{r Wards - pct increase}
# assess pct. increase
denominator <- cumsum(H.fit[[2]]) #calculates the cumulative sum of the heights (or distances) at which each clustering step takes place. In Ward's method, these heights represent the increase in the total within-cluster variance when two clusters are merged. H.fit[[2]] contains the heights of the merges at each step.

length(denominator) <- nobs-2 #length of the denominator vector is set to be one less than the number of observations in the data because the hierarchical clustering process involves n-1 merge steps.

denominator <- c(1,denominator) #A '1' is prepended to the denominator vector. This ensures that when calculating percentage increases, the first step (where no previous clustering has occurred) is properly handled by having a base of '1' to compare against.

pct <- H.fit[[2]]/denominator #calculates the percentage increase in the height at each step of the clustering process by dividing the height of each merge by the cumulative sum up to the previous step. This percentage increase can be interpreted as how much 'worse' the clustering gets at each step, as measured by Ward's distance metric.

tail(pct,n=10) #displays the last 10 values of the pct vector. These values are the percentage increases for the final 10 steps in the hierarchical clustering process.
```
NOT understand how we assess that 4 clusters is optimal from this percentage vector???

```{r wards - create cluster groups}
# apart from going from 2-1 the largest jump is from 4-3, we stop just before
grp <- as.factor(cutree(H.fit,k=4))
table(grp) #majority is in first cluster having 49 obs
```

```{r Wards - Dendrogram red borders}
# illustrate a 4 cluster solution
plot(H.fit)
rect.hclust(H.fit,k=4,border="red") #smart plot
```


```{r Wards - Assess cluster groups}
# assess outcome
aggregate(HBAT[,hbat],list(grp),mean)
```
Here it is seen that x6 Product quality is
- scoring highest in Cluster 2
- scoring lowest in Cluster 3 (etc.)
Here it is seen that x8 Technical Support is
- scoring highest in Cluster 2
- scoring lowest in Cluster 4 (etc.)

Group 1 has observations yielding high impact from x6 Product Quality and low impact from x8 Technical Support
Group 2 has observations yielding high impact from x6 Product Quality and high impact from x8 Technical Support (etc.)

```{r Wards - ANOVA on variables}
summary(aov(x6~grp,data=HBAT))
summary(aov(x8~grp,data=HBAT))
summary(aov(x12~grp,data=HBAT))
summary(aov(x15~grp,data=HBAT))
summary(aov(x18~grp,data=HBAT))
```
The ANOVA shows, that all variables except x18 Delivery speed have significant difference within the mean of the groups.
eg. x6 has means which differs in the group, as this group may be more volatile due to the scores given in each observation in the group
eg. x18 has means which is quite close to each other, yeilding a score where the observations are quite similar

## Complete-linkage method - Hierarchical clustering
-	Complete-linkage/farthest-neighbor measures the distance between clusters as the maximum of the distance between all possible pairs of objects in the two clusters – tends to find compact clusters with equal diameters
-	These methods only depend on the ordinal properties of the distances
```{r Complete-linkage method - Dendrogram }
# Complete linkage
H.fit <- hclust(dist,method="complete")
# draw a dendogram
plot(H.fit)
```


```{r Complete - pct increase }
# assess pct. increase
denominator <- cumsum(H.fit[[2]]) #calculates the cumulative sum of the heights (or distances) at which each clustering step takes place. In Ward's method, these heights represent the increase in the total within-cluster variance when two clusters are merged. H.fit[[2]] contains the heights of the merges at each step.

length(denominator) <- nobs-2 #length of the denominator vector is set to be one less than the number of observations in the data because the hierarchical clustering process involves n-1 merge steps.

denominator <- c(1,denominator) #A '1' is prepended to the denominator vector. This ensures that when calculating percentage increases, the first step (where no previous clustering has occurred) is properly handled by having a base of '1' to compare against.

pct <- H.fit[[2]]/denominator #calculates the percentage increase in the height at each step of the clustering process by dividing the height of each merge by the cumulative sum up to the previous step. This percentage increase can be interpreted as how much 'worse' the clustering gets at each step, as measured by Ward's distance metric.

tail(pct,n=10) ##displays the last 10 values of the pct vector. These values are the percentage increases for the final 10 steps in the hierarchical clustering process.
```
NOT understand how we assess that 4 clusters is optimal from this percentage vector???


```{r Complete - create cluster groups }
# results from Ward's method are supported -
grp <- as.factor(cutree(H.fit,k=4))
table(grp) #majority is in second cluster having 33 obs
```

```{r Complete - Dendrogram red borders }
# but size of the clusters differ
plot(H.fit)
rect.hclust(H.fit,k=4,border="red")
```


```{r Complete - Assess cluster groups }
# assess outcome
aggregate(HBAT[,hbat],list(grp),mean)
```
Here it is seen that x6 Product quality is
- scoring highest in Cluster 2
- scoring lowest in Cluster 4 (etc.)
Here it is seen that x8 Technical Support is
- scoring highest in Cluster 2
- scoring lowest in Cluster 1 (etc.)

Group 1 has observations yielding high impact from x6 Product Quality and low impact from x8 Technical Support
Group 2 has observations yielding high impact from x6 Product Quality and medium impact from x8 Technical Support (etc.)

```{r Complete - Dendrogram red borders }
summary(aov(x6~grp,data=HBAT))
summary(aov(x8~grp,data=HBAT))
summary(aov(x12~grp,data=HBAT))
summary(aov(x15~grp,data=HBAT))
summary(aov(x18~grp,data=HBAT))
```
The ANOVA shows, that all variables except x18 Delivery speed have significant difference within the mean of the groups.
eg. x6 has means which differs in the group, as this group may be more volatile due to the scores given in each observation in the group
eg. x18 has means which is quite close to each other, yeilding a score where the observations are quite similar

- IT IS RECOOMENDED THAT A COMBINATION OF THE HIERARCHICAL AND NONHIERARCHICAL METHODS IS USED
o	Use the hierarchical methods to get a qualified estimate on the number of clusters
o	Use the nonhierarchical methods to determine the cluster affiliation (tilknytning) for each object

BOOKMARK 15/4-24
--NOT FINISH--
```{r Complete - Dendrogram red borders }
# use NbClust
res<-NbClust(HBAT[,hbat], distance = "euclidean", min.nc=2, max.nc=8, 
             method = "ward.D", index = "all") #go with the printed number of clusters
res$All.index
res$Best.nc
```

#Non-hierarchical clusters

```{r Complete - Dendrogram red borders }
## Nonhierarchical clustering
set.seed(4118)
NH.fit<-kmeans(HBAT[,hbat],4,nstart=25)
print(NH.fit)
grp <- as.factor(NH.fit[[1]])
table(grp)
# assess outcome
aggregate(HBAT[,hbat],list(grp),mean)
summary(aov(x6~grp,data=HBAT))
summary(aov(x8~grp,data=HBAT))
summary(aov(x12~grp,data=HBAT))
summary(aov(x15~grp,data=HBAT))
summary(aov(x18~grp,data=HBAT))
# snake plot
matplot(t(NH.fit[[2]]),type="l")
# criterion validity
aggregate(HBAT[,c("x19","x20","x21","x22")],list(grp),mean)
summary(aov(x19~grp,data=HBAT))
summary(aov(x20~grp,data=HBAT))
summary(aov(x21~grp,data=HBAT))
summary(aov(x22~grp,data=HBAT))
# profiling
tbl <- table(HBAT$x1,grp)
round(100*prop.table(tbl,2))
chisq.test(tbl)
tbl <- table(HBAT$x2,grp)
round(100*prop.table(tbl,2))
chisq.test(tbl)
tbl <- table(HBAT$x3,grp)
round(100*prop.table(tbl,2))
chisq.test(tbl)
tbl <- table(HBAT$x4,grp)
round(100*prop.table(tbl,2))
chisq.test(tbl)
tbl <- table(HBAT$x5,grp)
round(100*prop.table(tbl,2))
chisq.test(tbl)


## Toy example
inc <- c(5,6,15,16,25,30)
edu <- c(5,6,14,15,20,19)
toy <- data.frame(inc,edu)
toy.dist <- dist(toy,method="euclidean")
toy.dist2 <- toy.dist^2
toy.H.single <- hclust(toy.dist2,method="single")
# the actual amalgamation
toy.H.single$merge
# the associated increase in distance/heterogeneity
toy.H.single$height
# a permutation of the original observations suitable for plotting
toy.H.single$order
# plot
plot(toy.H.single)

# other linkage possibilities
toy.H.complete <- hclust(toy.dist,method="complete")
toy.H.average <- hclust(toy.dist,method="average")
toy.H.centroid <- hclust(toy.dist,method="centroid")
# Wards method
toy.H.ward <- hclust(toy.dist2,method="ward.D")

## Distance vs shape
scores<-matrix(c(21,34,17,42,62,75,58,85),nrow=4,byrow=F)
matplot(scores,type="l")
dist(t(scores),method="euclidean")
cor(scores)

```

#II Segmentation
## McD Example

```{r 20.segmentation}
# R script for the analysis of the HBAT example in the model-based clustering
# lecture
library(readstata13)
rm(list=ls())

## Preparation
HBAT <- read.dta13("c:/users/au78328/OneDrive - Aarhus Universitet/home/customer_analytics/s24/analysis/hbat.dta")
# first we make our own id variable based on the original order
nobs <- nrow(HBAT)
HBAT$id <- seq(1,nobs)
# pick the variables for cluster analysis and summarize them
hbat <- c("x6","x8","x12","x15","x18")
summary(HBAT[,hbat])
apply(HBAT[,hbat],2,sd)
# multicollinearity should not be a problem
cor(HBAT[,hbat])
# which is confirmed
# look for outliers
dev <- t(t(HBAT[,hbat])-apply(HBAT[,hbat],2,mean))
dev2 <- dev^2
sumdev2 <- rowSums(dev2)
sort(sqrt(sumdev2))
# observations 6 and 87 are candidates for deletion
HBAT <- subset(HBAT,id!="6"&id!="87")
nobs <- nrow(HBAT)
HBAT$id <- seq(1,nobs)

## Model-based clustering
library("mclust")
(mc <- Mclust(HBAT[,hbat]))
# Assess different solutions
plot(mc, HBAT[,hbat], what = "BIC", col = "black")

# Summarize
summary(mc)

# Look at a 4-cluster solution
mc4 <- Mclust(HBAT[,hbat],G=4)
summary(mc4)

```
--NOT FINISH--

#III Segmentation
## McD Example

```{r dataload McD}
# R script for the analysis of the McDonald's example in the segmentation lecture
#install.packages("flexclust")
#install.packages("partykit")
#install.packages("clue") #- should be installed as part of the flexclust but isn't
library("flexclust")
library("partykit")
library("clue")
rm(list=ls())

## Preparation - Read in data
library(readr)
mcdonalds <- read_csv("Datasets/mcdonalds.csv")
```


```{r data insights}
# Look into content
names(mcdonalds)
dim(mcdonalds)
head(mcdonalds, 3) #showing the first 3 rows in dataset mcdonalds
```


```{r Discretize Yes:1 + Column means }
# Change Yes/No to 1/0
MD.x <- as.matrix(mcdonalds[, 1:11]) #function converts this selected subset of the data frame into a matrix called MD.x. mcdonalds[, 1:11] selects all rows and the first 11 columns of the mcdonalds data frame.
MD.x <- (MD.x == "Yes") + 0 # TRUE can be coerced to 1 and FALSE to 0. Therefore, adding 0 (+ 0) to the logical matrix effectively converts all TRUE values to 1s and all FALSE values to 0s.

## Exploring data
# What fraction sees McDonald's possessing each of the attributes
round(colMeans(MD.x), 2) #rounding the column means to two decimals
```


```{r PCA}
# PCA and perceptual map
MD.pca <- prcomp(MD.x)
summary(MD.pca)
```
PCA Principal Component analysis:
- The idea behind PCA is to reduce the dimensionality of the dataset by transforming the original variables into a new set of variables (the principal components) which are uncorrelated, and which are ordered so that the first few retain most of the variation present in all of the original variables.
- The greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.
Standard Deviation:
- This indicates how much variance is captured by each principal component. A higher standard deviation means that component accounts for a larger variance within the dataset.
Proportion of Variance:
- squared standard deviation for each component divided by the sum of squared standard deviations of all components. It tells us the proportion of the dataset’s total variance that is captured by each principal component.

```{r Rotation/Loadings PCA}
print(MD.pca, digits = 2)
```
Rotation matrix:
- Each row corresponds to one of the original variables (attributes like "yummy", "convenient", "spicy", etc.), and each column corresponds to a principal component (PC1 to PC11). The values in the matrix show how each attribute contributes to each principal component.
Example (original variable yummy on PC1):
- The attribute "yummy" has a strong positive loading on PC1 (0.4769) and a strong negative loading on PC9 (-0.5724), which means "yummy" is an important attribute that defines the direction of PC1 and PC9.

```{r PCA loading-plot}
plot(predict(MD.pca), col = "grey")
projAxes(MD.pca)
```
- dots represent the data points projected onto the first two principal components, PC1 and PC2. This provides a two-dimensional summary of the data, where similar data points are clustered together, and different ones are farther apart.
- arrows (or vectors) represent the loadings of each attribute (like "yummy", "cheap", "expensive", etc.), which point in the direction of the maximum variance for that attribute. The length of an arrow indicates the strength of the attribute's contribution to the two principal components displayed.
- Attributes that are close together are positively correlated, while those that are on opposite ends of the plot are negatively correlated.
- Attributes that are close to the origin (center of the plot) have smaller effects on the variation explained by the first two principal components, while those further out are more influential.

It is seen that "tasty" and "yummy" point in a similar direction, suggesting they contribute similarly to the dataset's variance and are likely correlated. Conversely, "expensive" points in a roughly opposite direction to "cheap", indicating they are negatively correlated. The plot also suggests that attributes like "disgusting" and "fattening" may not be strongly related to the primary variance within the data, as their vectors are shorter and closer to the center of the plot.


```{r k-means Within-cluster distances Plot }
## Extracting segments
# k-means
set.seed(28)
MD.km28 <- stepFlexclust(MD.x, 2:8, nrep = 10,verbose = FALSE) #compute k-means clustering on the matrix MD.x. The function is likely a stepwise routine that evaluates the optimal number of clusters ranging from 2 to 8.
#nrep = 10 indicates that each k-means operation is repeated 10 times to avoid local minima. 
#verbose = FALSE suppresses the printing of additional output during the computations

plot(MD.km28, xlab = "number of segments")
```
Plot: 
- bar chart that displays the sum of within-cluster distances for each number of clusters tested (from 2 to 8). Ideally, one looks for a "elbow" in the plot where the rate of decrease sharply changes, suggesting a natural grouping in the data. This "elbow" is often considered as an indicator of the optimal number of clusters to use.

Bootstrapping method for evaluating k-means clustering along with a boxplot visualizing the results
```{r Adj Rand index - bootstrap evaluating k-means}
# Segment stability within the same number of segments
#long compuation time!
set.seed(1234)
MD.b28 <- bootFlexclust(MD.x, 2:8, nrep = 10,nboot = 100) #a bootstrapping method for k-means clustering on the dataset MD.x
#nrep = 10 parameter indicates that the clustering should be repeated 10 times for stability, '
#nboot = 100 indicates that 100 bootstrap resamples should be used to assess the stability of the clustering solution.

plot(MD.b28, xlab = "number of segments",ylab = "adjusted Rand index")
```
Boxplot 
- visualizes the distribution of the adjusted Rand index for each number of segments (2 through 8). This plot can help determine the number of clusters that yield the most stable and consistent partitioning of the data by looking for the number of segments that consistently show high adjusted Rand index values
- observe the median, spread, and outliers of the adjusted Rand index across the different cluster solutions. The choice of the optimal number of segments might be influenced by looking for the highest median adjusted Rand index with the smallest interquartile range, indicating a stable and reliable clustering solution.
From this boxplot, a 4-cluster solution is evaluated as the most optimal amount of clusters: adjusted rand index= approx 0.75, no outliers etc?

Gorge plots -	For well-separated segments there should mainly be many low and many high similarity measures – hence the name (gorge/kløft)
```{r Gorge plot}
MD.km28 <- relabel(MD.km28) #apply the reLabel function to the clustering object MD.km28. This function is typically used to reorder the labels of the clusters for clarity or based on some criteria, although the specific behavior depends on the implementation within the package being used

histogram(MD.km28[["4"]], data = MD.x, xlim = 0:1) #using 4 clusters in this specific example
#xlim = 0:1 sets the limits for the x-axis from 0 to 1, assuming the similarity scores are normalized to this range.
```
Gorge plots (histograms):
- These histograms can help in interpreting the structure of the clusters. For example, a cluster with many high similarity scores (bars towards the right of the histogram) contains data points that are very similar to each other, indicating a tight and well-defined cluster. Conversely, a cluster with scores spread across the range might be less well-defined or could contain subgroups.
- the y-axis is labeled "Percent of Total", indicating that the histograms show the distribution of the similarity scores as a percentage of the total number of scores within each cluster. 
- the x-axis is labeled "similarity", denoting the range of similarity scores from 0 to 1.

SLS_A plot - Segment Level Stability Across solutions - plot
```{r SLS_A plot - Segment Level Stability Across solutions}
# Segment level stability across solutions
slsaplot(MD.km28)

```
SLS_A plot:
- The links (or bands) between the nodes show how many data points from each cluster in one solution correspond to clusters in another solution. In this case, it likely shows how segments in a lower number of clusters (on the left) are combined or split as the number of clusters increases (moving to the right).
- thickness of the bands is proportional to the number of data points that follow that path, illustrating the size of the clusters and the movement of data points between them. A stable clustering will have stronger, more direct paths, while a less stable one will show a more complex network of paths, indicating that the data points are being reassigned to different clusters across solutions.
- This visualization is valuable for assessing the robustness of clustering. If many data points maintain their cluster membership across different solutions, it indicates that the clustering is stable and likely meaningful. If there is a lot of reassignment across solutions, it suggests that the cluster structure is not as clear-cut.


```{r Specific X-cluster(4) - }
# Save the four-segment solution
MD.k4 <- MD.km28[["4"]] #defining the amount of clusters
MD.r4 <- slswFlexclust(MD.x, MD.k4) #function slsWFlexclust is applied to the dataset MD.x and the four-cluster solution MD.k4.

plot(MD.r4, ylim = 0:1, xlab = "segment number",ylab = "segment stability")
```
Specific X-cluster boxplot:
- boxplot shows the distribution of stability values for each of the four segments. In a boxplot, the box represents the interquartile range (IQR), which contains the middle 50% of the data. The line inside the box represents the median value. The whiskers represent the range of the data within 1.5 times the IQR above and below the box. Points outside this range are considered outliers and are plotted as individual points.
e.g.:
- Segment 1 seems to have higher median stability and less variability than the other segments, as shown by the narrow box.
- Segment 4 shows a lower median stability and more variability in stability, as indicated by the wider box and the presence of outliers.
- Segments 2 and 3 have moderate stability with some variability, as indicated by their median and the spread of the IQR.


```{r Segment Profile Plot - Profiling segments}
MD.vclust <- hclust(dist(t(MD.x))) #performs hierarchical clustering on the transpose of the matrix MD.x. Hierarchical clustering is a method of cluster analysis which seeks to build a hierarchy of clusters. The dist function calculates the distance matrix between the rows of MD.x (which become columns after the transpose operation), and hclust is then used to create a hierarchical clustering from that distance matrix. The result is stored in MD.vclust.

barchart(MD.k4, shade = TRUE,which = rev(MD.vclust$order)) #generates a barchart using the object MD.k4, which likely contains the clustering output from a previous k-means analysis. 
#shade = TRUE argument adds shading to the bars to give a visual indication of significance or magnitude. 
#which = rev(MD.vclust$order) argument reorders the barchart to match the order of clusters determined by the hierarchical clustering object MD.vclust. This reordering is done in reverse (rev) to perhaps match a specific ordering convention or for better visual clarity.
```
Segment profile plot:
- a set of barcharts that profile attributes across four different clusters. For each cluster, the attributes are listed along the y-axis, and the x-axis represents some measure related to the attribute, typically its frequency or importance within the cluster. 
- each bar might represent the mean value of the attribute within the cluster, and the red dot could indicate the median value or a special data point like an outlier.
- the numbers at the top of each barchart (e.g., Cluster 1: 499 (34%)) likely indicate the number of observations within each cluster and the percentage of the total dataset that each cluster represents.

Segmentation separation plot: 
- scatterplot for visualizing the separation of segments (or clusters) that were identified in a dataset, overlaid with the loadings of the attributes on the first two principal components from a PCA analysis.
```{r # Segment separation plot }
plot(MD.k4, project = MD.pca, data = MD.x,hull = FALSE, simlines = FALSE,
     xlab = "principal component 1",ylab = "principal component 2") #generates a scatter plot of the clusters defined in MD.k4, projecting the data points onto the first two principal components from MD.pca. 
#data = MD.x argument indicates the dataset used for the PCA. 
#hull = FALSE parameter suggests that convex hulls are not drawn around the clusters
#simlines = FALSE means that similarity lines (often used to connect similar items) are not drawn.

projAxes(MD.pca) # function, projAxes, is typically used to overlay the principal component loadings onto the plot. In this context, it probably adds the vectors that represent the directions and magnitudes of the original variables (attributes like "tasty", "cheap", "expensive", etc.) in the space defined by the first two principal components.
```
Segmentation separation plot:
- shows the data points from the dataset, with each point representing a single observation and the shape/color likely corresponding to the cluster to which the observation has been assigned.
- includes vectors that represent how each attribute aligns with the principal components. For instance, an attribute pointing towards the right on the x-axis (principal component 1) has a strong positive loading on that component, whereas an attribute pointing upwards on the y-axis (principal component 2) has a strong positive loading on the second component.
- helps to interpret the PCA by showing which attributes are most characteristic of each principal component, and also demonstrates the relationships between the clusters and the attributes.

Mosaic plot - Describing segments:
- extracting membership of segments for each consumer and creating a mosaic plot to describe the segments in terms of consumer opinions
```{r Mosaic plot - Describing segments }
# Extract the segment membership for each consumer
mcdonalds$k4 <- clusters(MD.k4) # extracts the cluster membership for each data point from the MD.k4 object, which presumably contains a k-means clustering result. The cluster membership for each data point (each consumer) is then stored in a new column $k4 of the mcdonalds data frame.

# Prepare the like/hate variable
mcdonalds$like <- factor(mcdonalds$Like,levels=c("I love it!+5","+4","+3","+2","+1","0","-1","-2","-3","-4","I hate it!-5")) #converts the like column of the mcdonalds data frame into a factor with ordered levels. These levels represent a scale of preference, from "I love it!+5" to "I hate it!-5", suggesting that there is an original numerical scale of -5 to +5, where +5 signifies strong liking and -5 signifies strong disliking.

# Make mosaic plot of like/hate
mosaicplot(table(mcdonalds$k4, mcdonalds$like), shade = TRUE,main = "", 
           xlab = "segment number") #mosaic plot is created from the cross-tabulation of mcdonalds$k4 and mcdonalds$like. The shade = TRUE argument indicates that the cells of the mosaic plot will be shaded according to the standardized residuals, which highlight the cells that have higher or lower counts than expected under independence.
```
Mosaic plot - Describing segments:
- each column represents a segment, and each color-coded row represents a level of the "like" variable. 
- the size of each colored block within the columns is proportional to the count of respondents in each like/dislike level for each segment. 
- this plot is a visual representation of the distribution of the like/dislike levels across different segments.


```{r Gender - Boxplot + Mosaic plot}
mosaicplot(table(mcdonalds$k4, mcdonalds$Gender), shade = TRUE)

boxplot(mcdonalds$Age ~ mcdonalds$k4,varwidth = TRUE, notch = TRUE) # formula indicates that the boxplot should compare the age variable (likely the age of the customers) across the different clusters specified in mcdonalds$k4.
#varwidth = TRUE: This argument specifies that the width of the boxplots should be proportional to the square root of the number of observations in the cluster. This helps visualize the relative sizes of the clusters.
#notch = TRUE: This argument adds notches to the boxplots. Notches are useful for assessing the significance of the difference between medians; if the notches of two boxes do not overlap, this suggests that the medians are significantly different.
```
Gender mosaic plot:
- same structure as "describing segments - mosaic plot"
- Distribution of gender in each of the segments/clusters

Gender Box-plot
- y-axis represents the ages of the respondents. 
- the variation in box widths suggests that the number of respondents varies across the clusters. 
- the presence of notches around the medians indicates where the medians lie and whether they might be significantly different from one another.

BOOKMARK 22/4-24 


```{r 21 segmentation}
## Predict segment 3 membership using a classification tree
# Prepare visit frequency variable
mcdonalds$visitfrequency <- factor(mcdonalds$VisitFrequency,
                                   levels=c("Never","Once a year","Every three months","Once a month","Once a week","More than once a week"))
tree <- ctree(factor(k4==3) ~ like+Age+visitfrequency+factor(Gender),
              data=mcdonalds)
plot(tree)
```


```{r 21 segmentation}
## Selecting (the) target segment(s)
# Visit frequency is scored as 1=Never,...,6=More than once a week, 
# not strictly aligned with the ratio properties hidden in the scale
visit <- tapply(as.numeric(mcdonalds$visitfrequency),mcdonalds$k4,mean)
visit
```


```{r 21 segmentation}
# Turn the current like scale upside down and recenter 
# (1=I love it,...,11=I hate it -> 5=I love it,...,-5=I hate it)
like <- tapply(-as.numeric(mcdonalds$like)+6,mcdonalds$k4,mean)
like
```


```{r 21 segmentation}
# Create the gender variable
female <- tapply((mcdonalds$Gender=="Female")+0,mcdonalds$k4,mean)
female
```


```{r 21 segmentation}
# Make segment evaluation plot
plot(visit, like, cex = 10 * female,xlim = c(2, 4.5), ylim = c(-3, 3))
text(visit, like, 1:4)

```

#IV Segmentation
## McD Example

```{r}
# R script for the analysis of the McDonald's example in LCA I lecture
library("flexclust")
library("flexmix")
rm(list=ls())

## Preparation
# Read in data
mcdonalds <- read.csv("c:/users/au78328/OneDrive - Aarhus Universitet/home/customer_analytics/s24/analysis/mcdonalds.csv")
# Look into content
names(mcdonalds)
dim(mcdonalds)
head(mcdonalds, 3)
# Change Yes/No to 1/0
MD.x <- as.matrix(mcdonalds[, 1:11])
MD.x <- (MD.x == "Yes") + 0

## Exploring data
# What fraction sees McDonald's possessing each of the attributes
round(colMeans(MD.x), 2)
# PCA and perceptual map
MD.pca <- prcomp(MD.x)
summary(MD.pca)
print(MD.pca, digits = 1)
plot(predict(MD.pca), col = "grey")
projAxes(MD.pca)

## Extracting segments
# k-Means
set.seed(28)
MD.km28 <- stepFlexclust(MD.x, 2:8, nrep = 10,verbose = FALSE)
plot(MD.km28, xlab = "number of segments")
# Segment stability within the same number of segments
set.seed(1234)
MD.b28 <- bootFlexclust(MD.x, 2:8, nrep = 10,nboot = 100)
plot(MD.b28, xlab = "number of segments",ylab = "adjusted Rand index")
# Gorge plot
MD.km28 <- relabel(MD.km28)
histogram(MD.km28[["4"]], data = MD.x, xlim = 0:1)
# Segment level stability across solutions
slsaplot(MD.km28)
# Save the four-segment solution
MD.k4 <- MD.km28[["4"]]
MD.r4 <- slswFlexclust(MD.x, MD.k4)
plot(MD.r4, ylim = 0:1, xlab = "segment number",ylab = "segment stability")
# mixture distribution
set.seed(1234)
MD.m28 <- stepFlexmix(MD.x ~ 1, k = 2:8, nrep = 10, model = FLXMCmvbinary(), 
                      verbose = FALSE)
MD.m28
plot(MD.m28,ylab = "value of information criteria (AIC, BIC, ICL)")
MD.m4 <- getModel(MD.m28, which = "4")
table(kmeans = clusters(MD.k4),mixture = clusters(MD.m4))
MD.m4a <- flexmix(MD.x ~1, cluster = clusters(MD.k4),model = FLXMCmvbinary())
table(kmeans = clusters(MD.k4),mixture = clusters(MD.m4a))
logLik(MD.m4a)
logLik(MD.m4)
```


#V Segmentation
## McD Example

```{r}
# R script for the analysis of the McDonald's example in LCA II lecture
library(flexmix)
library("flexclust")
rm(list=ls())

## Preparation
# Read in data
mcdonalds <- read.csv("c:/users/au78328/OneDrive - Aarhus Universitet/home/customer_analytics/s24/analysis/mcdonalds.csv")
# Look into content
names(mcdonalds)
dim(mcdonalds)
head(mcdonalds, 3)
# Change Yes/No to 1/0
MD.x <- as.matrix(mcdonalds[, 1:11])
MD.x <- (MD.x == "Yes") + 0

## Exploring data
# What fraction sees McDonald's possessing each of the attributes
round(colMeans(MD.x), 2)
# PCA and perceptual map
MD.pca <- prcomp(MD.x)
summary(MD.pca)
print(MD.pca, digits = 1)
plot(predict(MD.pca), col = "grey")
projAxes(MD.pca)

## Extracting segments
# Mixture regression
# Need to fix the representation of like and use "like" (not "Like") onwards
mcdonalds$like <- factor(mcdonalds$Like,levels=c("I love it!+5","+4","+3","+2","+1","0","-1","-2","-3","-4","I hate it!-5"))
rev(table(mcdonalds$like))
--
mcdonalds$like.n <- 6 - as.numeric(mcdonalds$like)
table(mcdonalds$like.n)
--
# Prepare the regression function for the finite mixture of linear regression models
f <- paste(names(mcdonalds)[1:11], collapse = "+")
f <- paste("like.n ~ ", f, collapse = "")
f <- as.formula(f)
f
--
set.seed(1234)
MD.reg2 <- stepFlexmix(f, data = mcdonalds, k = 2,nrep = 10, verbose = FALSE)
MD.reg2
--
MD.ref2 <- refit(MD.reg2)
summary(MD.ref2)
plot(MD.ref2, significance = TRUE)
```


